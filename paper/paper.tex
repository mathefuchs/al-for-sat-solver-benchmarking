\documentclass[runningheads]{llncs}

\usepackage[ruled,vlined,linesnumbered,norelsize]{algorithm2e}
\usepackage{amsmath,amssymb}
\usepackage[T1]{fontenc} % font encoding; recommended by LNCS template
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[misc]{ifsym} % letter symbol to mark corresponding author
\usepackage{pifont}
\usepackage{siunitx}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage[table]{xcolor}
\usepackage{tikz} % has to be loaded after "xcolor" here, else "option clash"

\newcommand{\cmark}{\ding{51}} % symbols for table comparing benchmarking approaches
\newcommand{\xmark}{\ding{55}}

\DontPrintSemicolon % configuration for algorithm2e
\def\NlSty#1{\textnormal{\fontsize{8}{10}\selectfont{}#1}}
\SetKwSty{texttt}
\SetCommentSty{emph}

\renewcommand\UrlFont{\color{blue}\rmfamily} % link coloring according to LNCS template

\begin{document}

\title{Active Learning for SAT Solver Benchmarking}

\author{
  Tobias Fuchs\orcidID{0000-0001-9727-2878} (\Letter) \and \\
  Jakob Bach\orcidID{0000-0003-0301-2798} \and \\
  Markus Iser\orcidID{0000-0003-2904-232X}
}

\institute{
  Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany \\
  \email{info@tobiasfuchs.de, jakob.bach@kit.edu, markus.iser@kit.edu}
}

\authorrunning{T. Fuchs et al.} % according to sample paper of template: first names are abbreviated; if there are more than two authors, 'et al.' is used.

\maketitle

\begin{abstract}
  Benchmarking is one of the most important phases when developing algorithms.
  This also applies to solutions to the SAT (propositional satisfiability) problem.
  Benchmark selection chooses representative instances from a pool of instances that reliably discriminate SAT solvers based on their runtime.
  In this paper, we present a dynamic benchmark selection approach based on active learning.
  Our approach predicts the rank of a new solver among its competitors with minimum running time and maximum rank prediction accuracy.
  We evaluated this approach using the Anniversary Track dataset from the 2022 SAT Competition.
  Our selection approach can predict the rank of a new solver after about \SI{10}{\%} of the time it would take to run the solver on all instances of this dataset, with a prediction accuracy of about \SI{92}{\%}. 
  In this paper, we also discuss the importance of instance families in the selection process. 
  Overall, our tool provides a reliable way for SAT solver engineers to efficiently determine the performance of a new SAT solver.

  \keywords{Propositional satisfiability \and Benchmarking \and Active learning}
\end{abstract}


\section{Introduction}
\label{sec:intro}

One of the main phases of algorithm engineering is benchmarking.
This is also true for the propositional satisfiability problem (SAT), the archetypal $\mathcal{NP}$-complete problem.
Benchmarking is, however, quite expensive regarding the runtime of experiments.
Competitive SAT solvers are the result of extensive experimentation with a variety of ideas and considerations~\cite{FroleyksHIJS21,sat2022}.
While the field of SAT solver benchmarking is well established, traditional benchmark selection approaches do not optimize for benchmark runtime.
Instead, the primary goal of traditional approaches is to select a representative set of instances for ranking solvers with a scoring scheme~\cite{Gelder11,HoosKSS13}.
SAT~Competitions typically use the \mbox{PAR-2} score, i.e., the average runtime with a penalty of $2 \tau$ for timeouts with time-limit~$\tau$~\cite{FroleyksHIJS21}.

In this paper, we present a novel benchmark selection approach based on active learning.
Our approach can predict the rank of a new solver with high accuracy in only a fraction of the time needed to evaluate the full benchmark.
The problem we solve is specified in Definition~\ref{def:new-solver-problem}.

\begin{definition}[New-Solver Problem]
  Given solvers~$\mathcal{A}$, instances~$\mathcal{I}$, runtimes~$r\!: \mathcal{A} \times \mathcal{I} \rightarrow \left[0, \tau\right]$ with time-limit $\tau$, and a new solver $\hat{a} \notin \mathcal{A}$, incrementally select benchmark instances in $\mathcal{I}$ to maximize the confidence in predicting the rank of $\hat{a}$ while minimizing the total benchmark runtime.
  \label{def:new-solver-problem}
\end{definition}

Note that our scenario assumes that we know the runtimes of all solvers, except the new one, on all instances.
One could also imagine a collaborative filtering scenario, where runtimes are only partially known~\cite{misir2017data,misir2017alors}.

Our approach satisfies several desirable criteria for benchmarking:
Rather than outputting a binary classification, i.e., whether the new solver is worse than an existing solver or not, we provide a \emph{scoring} function that shows by which margin a solver is worse and how similar it is to existing solvers.
In particular, our approach enables \emph{ranking} the new solver amidst a set of existing solvers.
For this ranking, we do not even need to predict exact solver runtimes, which is trickier.
Further, we optimize the \emph{runtime} that our strategy needs to arrive at its conclusion.
We use instance and runtime \emph{features}.
Moreover, we select instances \emph{non-randomly} and \emph{incrementally}.
In particular, we consider runtime information from already done experiments when choosing the next.
By doing so, we can control the properties of the benchmarking approach, such as its required runtime.
Our approach is \emph{scalable} in that it ranks a new solver $\hat{a}$ among any number of known solvers $\mathcal{A}$.
In particular, we only subsample the benchmark once instead of comparing pairwise against each other solver~\cite{MatriconAFSH21}.
Since a new solver idea is rarely best on the first try, it is desired to rank it fast.
In this way, one can discard the new solver idea if it performs poorly across the board or may tweak it further if it shows promising results at least in some cases.

We evaluate our approach with the SAT Competition~2022 Anniversary Track benchmark~\cite{sat2022}, consisting of 5355~instances and complete runtimes of 28~solvers.
We perform cross-validation by treating each solver as new once and predicting these solvers' PAR-2 rank.
On average, our predictions reach about \SI{92}{\%} accuracy with only about \SI{10}{\%} of the runtime that would be needed to evaluate these solvers on the complete set of instances.

% TODO: insert URLs
All our source code\footnote{temporary, anonymized version for review: \url{xxx}} and and data\footnote{temporary, anonymized version for review: \url{xxx}} are available on GitHub.


\section{Related Work}

Benchmarking is not only of high interest in many fields but also an active research area on its own.
Recent studies show that benchmark selection is challenging for multiple reasons.
Biased benchmarks can easily lead to fallacious interpretations~\cite{abs-2107-07002}.
Also, benchmark selection has many movable parts, e.g., performance measures, aggregation, and handling of missing values.
Questionable research practices might modify these elements a-posteriori to fit expectations, biasing results~\cite{NiesslHWCB22}.
In the following, we discuss related work from the areas of static benchmark selection, algorithm configuration, incremental benchmark selection, and active learning.
Table~\ref{tab:requirements} compares the most relevant approaches.

\subsubsection{Static Benchmark Selection.}

Benchmark selection is essential for competitions, e.g., the SAT Competition.
In such competitions, the organizers define the rules for composing the benchmarks.
These selection strategies are mostly static, i.e., they do not depend on particular solvers to distinguish.
Balint et al. provide an overview of benchmark-selection criteria in different solver competitions~\cite{balint2015overview}.
Froleyks et al. describe benchmark selection as it is done in recent SAT~competitions~\cite{FroleyksHIJS21}.
Manthey and MÃ¶hle find that competition benchmarks might contain redundant instances and propose a feature-based approach to remove redundancy~\cite{manthey2016better}.
M{\i}s{\i}r presents a feature-based approach to reduce benchmarks by matrix factorization and clustering~\cite{misir2021benchmark}.

Hoos et al.~\cite{HoosKSS13} discuss which properties are most desirable when selecting SAT benchmark instances.
Selection criteria are instance variety to avoid over-fitting, adapted instance hardness (not too easy but also not too hard), and avoidance of duplicate instances.
To filter too similar instances, they use a distance-based approach with the SATzilla features~\cite{XuHHL08,features}.
The approach does, however, not optimize for benchmark \emph{runtime} and selects instances \emph{randomly}, apart from constraints on the instance hardness and feature distance.

\begin{table}[tbp]
  \centering
  \caption{Comparison of features of our benchmark-selection approach, the static benchmark-selection approach by Hoos~et~al.~\cite{HoosKSS13}, the algorithm configuration system SMAC~\cite{HutterHL11}, and the active-learning approaches by Matricon et al.~\cite{MatriconAFSH21}.
  }
  \label{tab:requirements}
  ~\\[1em]
  \begin{tabular}{
    m{0.32\textwidth}
    >{\centering\arraybackslash}m{0.16\textwidth}
    >{\centering\arraybackslash}m{0.16\textwidth}
    >{\centering\arraybackslash}m{0.16\textwidth}
    >{\centering\arraybackslash}m{0.16\textwidth}
  }
    \hline
    Feature & Hoos~\cite{HoosKSS13} & SMAC~\cite{HutterHL11} & Matricon~\cite{MatriconAFSH21} & Our approach \\
    \hline
    Ranking/Scoring & \cmark & \xmark & (\cmark) & \cmark \\
    Runtime Minimization & \xmark & \cmark & \cmark & \cmark \\
    Incremental/Non-Random & \xmark & \xmark & \cmark & \cmark \\
    Scalability & \cmark & \cmark & \xmark & \cmark \\
    \hline
  \end{tabular}
\end{table}

\subsubsection{Algorithm Configuration.}

Further related work can be found within the field of algorithm configuration~\cite{HoosHL21,Stutzle0P22}, e.g., the configuration system SMAC~\cite{HutterHL11}.
Thereby, the goal is to tune SAT solvers for a given sub-domain of problem instances.
Although this task is different from our goal, e.g., we do not need to navigate configuration space, there are similarities to our approach as well.
For example, SMAC also employs an iterative, model-based selection procedure, though for configurations rather than instances.
An algorithm configurator, however, cannot be used to \emph{rank/score} a new solver since algorithm configuration solemnly seeks to find the best-performing configuration.
Also, while using a model-based selection strategy to sample configurations, instance selection is made \emph{randomly}, i.e., without building a model over instances.

\subsubsection{Incremental Benchmark Selection.}

Matricon~et~al. present an incremental benchmark selection approach~\cite{MatriconAFSH21}.
Their \emph{per-set efficient algorithm selection problem} (PSEAS) is similar to our \emph{New-Solver Problem} (cf.~Definition~\ref{def:new-solver-problem}).
Given a pair of SAT solvers, they iteratively select a subset of instances until the desired confidence level is reached to decide which of the two solvers is better.
The selection of instances depends on the choice of the solvers to distinguish.
They calculate a scoring metric for all unselected instances, run the experiment with the highest score, and update the confidence.
Their approach ticks off most of our desired features in Table~\ref{tab:requirements}.
However, the approach only compares solvers binarily rather than providing a \emph{scoring}.
Thus, it is unclear how similar two given solvers are or on which instances they behave similarly.
Moreover, a significant shortcoming is the lacking \emph{scalability} with the number of solvers.
Comparing only pairs of solvers, evaluating a new solver requires sampling a separate benchmark for each existing solver.
In contrast, our approach allows comparing a new solver against a set of existing solvers by sampling only one benchmark.

\subsubsection{Active Learning.}

The posed \emph{New-Solver Problem} has stark similarities to the well-studied field of active learning (AL) within recommender systems, in particular the \emph{new-user problem}~\cite{RubensESK15}.
On the one hand, we want to maximize the utility an instance provides to our model, i.e., rank prediction accuracy, and on the other hand, minimize the cost, i.e., runtime, that is associated with its acquisition.
In contrast to traditional passive machine-learning methods with given instance labels, active learning allows for selecting instances for which to acquire labels.
AL algorithms can be categorized into \emph{synthesis-bas\-ed} \cite{0001AEMN22,GarzonMG22,2019gaal} and \emph{pool-bas\-ed} approaches \cite{GolbandiKL11,HarpaleY08,KorenBV09}.
While synthesis-based methods generate instances for labeling, pool-based methods rely on a fixed set of unlabeled instances from which to sample.

Recent synthesis-based methods within the field of SAT solving show how to generate problem instances with desired properties.
This goal is, however, orthogonal to ours~\cite{0001AEMN22,GarzonMG22}.
While those approaches want to generate instances on which a solver is good or bad, we want to predict whether a solver is good or bad on an existing benchmark.
Volpato and Guangyan use pool-based AL to learn an instance-specific algorithm selector~\cite{volpato2019active}.
Rather than benchmarking a solver's overall performance, their goal is to recommend the best solver out of a set of solvers for each SAT instance.

\section{Active Learning for SAT Solver Benchmarking}
\label{sec:main}

The outline of our benchmarking framework is depicted in Algorithm~\ref{algALBenchmark}. 
Given a set of known solvers $\mathcal{A}$, instances $\mathcal{I}$ and runtimes $r$, we first initialize a prediction model~$\mathcal{M}$ for the new solver $\hat a$ (Line~1).
The prediction model~$\mathcal{M}$ is used to repeatedly select an instance (Line~4) for benchmarking $\hat a$ (Line~5). 
The acquired result is subsequently used to update the prediction model~$\mathcal{M}$ (Line~7). 
When the stopping criterion is met (Line~3), we quit the benchmarking loop and predict the final score of $\hat a$ (Line~8). 
Algorithm~\ref{algALBenchmark} returns the predicted score of $\hat a$ as well as the acquired instances and runtime measurements (Line~9). 

Section~\ref{sec:main:model} describes the underlying prediction model~$\mathcal{M}$ and specifies how we may derive a solver ranking from it.
We discuss criteria for selecting instances in Section~\ref{sec:main:selection}.
Section~\ref{sec:main:stopping} concludes with possible stopping conditions.

\begin{algorithm}
  \caption{Incremental Benchmarking Framework}
  \label{algALBenchmark}

  \KwIn{Solvers $\mathcal{A}$, Instances $\mathcal{I}$, Runtimes $r : \mathcal{A} \times \mathcal{I} \rightarrow [0, \tau]$, Solver $\hat{a}$}
  \KwOut{Predicted Score of $\hat{a}$, Measured Runtimes $\mathcal{R}$}

  \BlankLine

  $\mathcal{M} \leftarrow \operatorname{initModel}\left(\mathcal{A},\, \mathcal{I},\, r,\, \hat{a}\right)$ \tcp*{cf. Section~\ref{sec:main:model}}
  
  \BlankLine
  $\mathcal{R} \leftarrow \emptyset$ \;
  \While(\tcp*[f]{cf. Section~\ref{sec:main:stopping}}){$\operatorname{not} \operatorname{stop}\left(\mathcal{M}\right)$}{
    $e \leftarrow \operatorname{selectNextInstance}\left(\mathcal{M}\right)$ \tcp*{cf. Section~\ref{sec:main:selection}}

    $t \leftarrow \operatorname{runExperiment}\left(\hat{a},\,  e\right)$  \tcp*{Runs $\hat{a}$ on $e$ with time-out $\tau$}

    $\mathcal{R} \leftarrow \mathcal{R} \cup \left\lbrace (e,\, t) \right\rbrace$

    \BlankLine
    $\operatorname{updateModel}\left(\mathcal{M},\, \mathcal{R}\right)$ \tcp*{cf. Section~\ref{sec:main:model}}
  }
  $s_{\hat a} \leftarrow \operatorname{predictScore}(\mathcal{M})$ \tcp*{cf. Section~\ref{sec:main:model}}
  
  \BlankLine
  \Return $(s_{\hat a}, \mathcal{R})$
\end{algorithm}


\subsection{Solver Model}
\label{sec:main:model}

Let SAT solvers $\mathcal{A}$, benchmark instances $\mathcal{I}$, and runtimes $r : \mathcal{A} \times \mathcal{I} \rightarrow \left[0, \tau\right]$ be given.
We denote the new solver to be ranked by $\hat a \not\in A$ and define $\hat A := A \cup \lbrace \hat a \rbrace$ to denote all solvers, including the new solver. 
Model initialization (Line~1) creates the prediction model and transforms runtimes, as discussed in the subsequent section.
No model training is needed at this point since we do not have any labeled data yet.
Later, model $\mathcal{M}$ provides a runtime-label prediction function $f : \mathcal{\hat A} \times \mathcal{I} \rightarrow \mathbb{R}$ that powers the decisions within our framework.
In particular, the model update (Line~7) uses all acquired runtimes to train $f$ with a standard supervised machine-learning methodology.
Training features are instance features and known runtimes $r$.
After applying the necessary transformations described in the subsequent section, we fit the prediction model using the transformed observations $\mathcal{R}$ as ground-truth labels.
Note that we build a new prediction model in each iteration since running experiments (Line~5) dominates the runtime of Algorithm~\ref{algALBenchmark} by magnitudes.
Finally, we predict the score of the new solver $\hat a$ in Line~8 with the prediction function~$f$.

\subsubsection{Runtime Transformation.}

We transform the runtimes into discrete runtime labels on a per-instance basis.
For each instance $e \in \mathcal{I}$, we use a clustering algorithm to assign the runtimes in $\bigl\{ r(a, e) \mid a \in A \bigr\}$ to one of $k$ clusters $C_1, \dots, C_k$ such that the fastest runtimes for instance $e$ are in cluster $C_1$ and the slowest are in cluster $C_{k-1}$.
Timeouts $\tau$ always form a separate cluster $C_{k}$.
The runtime transformation function $\gamma_k : {\mathcal{A} \times \mathcal{I}} \rightarrow \left\lbrace 1, \dots, k \right\rbrace$ is then specified as follows:
%
$$\gamma_k(a, e) = j ~\Leftrightarrow~ r(a, e) \in C_j$$
%
Given an instance $e \in \mathcal{I}$, a solver $a \in A$ belongs to the $\gamma_k(a, e)$-fastest solvers on instance $e$. 
In preliminary experiments, we achieved higher accuracy in predicting such discrete-runtime labels than in predicting raw runtimes.
Research on portfolio solvers has also shown that discretization works well in practice~\cite{CollauttiMMO13,NgokoCT19}.

\subsubsection{Ranking Solvers.}

To determine solver ranks, we use the transformed runtimes $\gamma_k(a, e)$ in the adapted scoring function $s_k : A \rightarrow [1, 2 \cdot k]$ as follows:
%
\begin{align}
  s_k(a) := \frac{1}{|\mathcal{I}|} \sum_{e \in \mathcal{I}} \gamma'_k(a, e)
  &&
  \gamma'_k(a, e) := \begin{cases}
    2 \cdot \gamma_k(a, e)   & \text{if } \gamma_k(a, e) = k\\
  \gamma_k(a, e)  & \text{otherwise}
  \end{cases}
  \label{eq:rankingeq}
\end{align}
%
I.e., we apply PAR-2 scoring, which is commonly used in SAT competitions~\cite{FroleyksHIJS21}, on the discrete labels.
The scoring function $s_k$ induces a ranking among solvers.


\subsection{Instance Selection}
\label{sec:main:selection}

Selecting an instance based on the model is a core functionality of our framework (cf. Algorithm~\ref{algALBenchmark}, Line~4).
In this section, we introduce our sampling strategies, which use the model's label-prediction function $f$ and are inspired by existing work within the realms of active learning~\cite{settles2009active}.
We implement an uncertainty and an information-gain sampling strategy.
These methods require the model's predictions to include probabilities for the $k$ discrete runtime labels induced by the clustering described in the preceding section.
Let \mbox{$f' : \mathcal{\hat A} \times \mathcal{I} \rightarrow \left[0, 1\right]^k$} denote this modified prediction function.

\subsubsection{Uncertainty Sampling.}

The uncertainty sampling strategy simply selects the instance closest to the model's decision boundary.
The set $\tilde{\mathcal{I}} \subseteq \mathcal{I}$ denotes the instances that have already been sampled.
%
\begin{equation*}
  \underset{e \in \mathcal{I} \setminus \tilde{\mathcal{I}}}{\arg\min} \left\lvert \frac{1}{k} - \max_{n \in \left\lbrace 1, \dots, k \right\rbrace} f'\!\left(\hat{a}, e\right)_{n} \right\rvert
\end{equation*}

\subsubsection{Information-Gain Sampling.}

The information-gain sampling strategy selects the instance with the highest expected entropy reduction regarding the runtime labels of the instance.
To be more specific, we select the instance $e \in \mathcal{I} \setminus \tilde{\mathcal{I}}$ that maximizes $IG(e)$, which is specified as follows:
%
\begin{equation*}
  \operatorname{IG}(e) := \operatorname{H}(e) - \sum_{n = 1}^{k} f'(\hat{a}, e)_{n} \operatorname{\hat H}_n(e)
\end{equation*}
%
Here, $\operatorname{H}(e)$ denotes the entropy of the runtime labels $\gamma(a, e)$ over all $a \in \mathcal{A}$ and $\operatorname{H}(e, n)$ denotes the entropy of these labels plus $n$ as the runtime label for $\hat{a}$.
The term $\operatorname{\hat H}_n(e)$ is computed for every possible runtime label $n \in \{1, \dots, k\}$.
By maximizing information gain, we select instances that identify solvers with similar behavior.

\subsection{Stopping Criteria}
\label{sec:main:stopping}

In this section, we present two dynamic stopping criteria, the Wilcoxon and the ranking stopping criterion (cf. Algorithm~\ref{algALBenchmark}, Line~3).

\subsubsection{Wilcoxon Stopping Criterion.}

The Wilcoxon stopping criterion stops the active-learning process when we are confident enough that the predicted runtime labels of the new solver are sufficiently different from the labels of the existing solvers.
This criterion is loosely inspired by Matricon et.~al.~\cite{MatriconAFSH21}.
We use the average $p$-value $W_{\hat{a}}$ of a Wilcoxon signed-rank test $w(S,P)$ of the two runtime label distributions $S=\{ \gamma(a, e) \mid e \in \mathcal{I} \}$ for an existing solver $a$ and \mbox{$P=\{ f(\hat a, e) \mid e \in \mathcal{I} \}$} for the new solver $\hat{a}$:
%
\begin{equation*}
  W_{\hat{a}} := \frac{1}{\lvert \mathcal{A} \rvert} \sum_{a \in \mathcal{A}} \operatorname{w}(S, P)
\end{equation*}
%
To improve the stability of this criterion, we use an exponential moving average to smooth out outliers and stop as soon as $W^{(i)}_{\exp}$ drops below a fixed threshold:
%
\begin{align*}
  W_{\exp}^{\left(0\right)} &:= 1\\
  W_{\exp}^{\left(i\right)} &:= \beta W_{\hat{a}} + \left(1 - \beta\right) W_{\exp}^{\left(i - 1\right)}
\end{align*}

\subsubsection{Ranking Stopping Criterion.}

The ranking stopping criterion is less sophisticated in comparison.
It stops the active-learning process if the ranking induced by the model's predictions (Equation~\ref{eq:rankingeq}) remains unchanged within the last $l$ iterations.
Note that the concrete values of the predicted score $s_{\hat a}$ might still change.
We are solemnly interested in the induced ranking in this case.


\section{Experimental Design}
\label{sec:exdesign}

Given all the previously presented instantiations for Algorithm~\ref{algALBenchmark}, this section briefly outlines our experimental design, including our evaluation framework, used data sets, hyper-parameter choices, and implementation details.

\subsection{Evaluation Framework}
\label{sec:exdesign:eval}

\begin{algorithm}[tb]
  \caption{Evaluation Framework}
  \label{alg:eval}

  \KwIn{Solvers $\mathcal{A}$, Instances $\mathcal{I}$, Runtimes $r : \mathcal{A} \times \mathcal{I} \rightarrow [0, \tau]$}
  \KwOut{Average Rank Accuracy $\bar{O}_{\operatorname{acc}}$, Average Fraction of Runtime $\bar{O}_{\operatorname{rt}}$}
  \BlankLine

  $O \leftarrow \emptyset$
  \BlankLine

  \For{$\hat{a} \in \mathcal{A}$}{
    $\mathcal{A}' \leftarrow \mathcal{A} \setminus \left\lbrace \hat{a} \right\rbrace$ \;
    $(s_{\hat a}, \mathcal{R}) \leftarrow \operatorname{runALAlgorithm}(\mathcal{A}', \mathcal{I}, r, \hat{a})$ \tcp*{Refer to Algorithm~\ref{algALBenchmark}}

  \BlankLine
  \tcp{Determine Rank Accuracy}
    $O_{\operatorname{acc}} \leftarrow 0$ \;    
    \For{$a \in \mathcal{A}$}{
      \If{$\bigl(s_k(a) - s_{\hat a}\bigr) \cdot \bigl(\operatorname{par_2}(a) - \operatorname{par_2}(\hat a)\bigr) > 0$}{
           $O_{\operatorname{acc}} \leftarrow O_{\operatorname{acc}} + \frac{1}{|\mathcal{A}|}$ \;
      }
    }
  
  \BlankLine
  \tcp{Determine Runtime Fraction}
  $r \leftarrow \sum\limits_{e \in \mathcal{I}} r(\hat a, e)$\;
  $O_{\operatorname{rt}} \leftarrow 0$\;
    \For{$e \in \mathcal{I}$}{
    \If{$\exists t, (e,t) \in \mathcal{R}$}{
      $O_{\operatorname{rt}} \leftarrow O_{\operatorname{rt}} + \frac{t}{r}$\;
    }
  }
  
  \BlankLine
    $O \leftarrow O \cup \bigl\{ ( O_{\operatorname{acc}},\, O_{\operatorname{rt}} ) \bigr\}$
  }

  \BlankLine
  $\bigl( \bar{O}_{\operatorname{acc}}, \bar{O}_{\operatorname{rt}} \bigr) \leftarrow \operatorname{average}(O)$ \;
  
  \BlankLine
  \Return $\bigl( \bar{O}_{\operatorname{acc}}, \bar{O}_{\operatorname{rt}} \bigr)$
\end{algorithm}

As already stated in the introductory section, this work addresses the \emph{New-Solver Problem} (cf.~Definition~\ref{def:new-solver-problem}).
We create a prediction model $\mathcal{M}$ as described in Section~\ref{sec:main:model} that provides us with a scoring function $s_k$.

To evaluate a concrete instantiation of Algorithm~\ref{algALBenchmark}, i.e., a concrete choice for all the sub-routines, we perform cross-validation on our set of solvers.
Algorithm~\ref{alg:eval} shows this.
That means that each solver plays the role of the new solver once (Line~2).
Note that the \emph{new} solver in each iteration is excluded from the set of solvers $\mathcal{A}$ to avoid data leakage (Line~3).
After running our active-learning framework for a solver $\hat{a}$ in Line~4, we compute the value of both our optimization goals:
First and foremost, we want to provide the engineer of new SAT solvers with an accurate ranking.
We define the \emph{ranking accuracy} $O_{\operatorname{acc}} \in \left[0, 1\right]$ (higher is better) by the fraction of pairs $\left(\hat{a}, a\right)$ for all $a \in \mathcal{A}$ that are decided correctly by the given ranking regarding the PAR-2 scores $\operatorname{par_2}$ (Lines~5-8).
Second, we also want to optimize for runtime.
The \emph{fraction of runtime} that the algorithm needs to arrive at its conclusion is denoted by $O_{\operatorname{rt}} \in \left[0, 1\right]$ (lower is better).
This metric puts the runtime summed over the sampled instances in relation to the runtime summed over all instances in the dataset (Lines~9-13).
Finally, we compute averages of the output metrics in Line~15 after we have collected all cross-validation results in Line~14.
Overall, we want to find an approach that maximizes
%
\begin{equation}
  O_\delta := \delta O_{\operatorname{acc}} + \left(1 - \delta\right) \left(1 - O_{\operatorname{rt}}\right) \enspace \textrm{,}
  \label{eq:opt}
\end{equation} 
%
whereby $\delta \in \left[0, 1\right]$ allows for linear weighting between the two optimization goals $O_{\operatorname{acc}}$ and $O_{\operatorname{rt}}$.
Plotting the approaches that maximize $O_\delta$ for all $\delta \in \left[0, 1\right]$ on a $O_{\operatorname{rt}}$-$O_{\operatorname{acc}}$-diagram provides us with a Pareto front of the best approaches for different optimization-goal weightings.

\subsection{Data}

In our experiments, we work with the dataset of the SAT~Competition~2022 Anniversary Track~\cite{sat2022}.
The dataset consists of 5355 instances with respective runtime data of 28 sequential SAT solvers.
We also use a database of 56 instance features\footnote{\url{https://gbd.iti.kit.edu/getdatabase/base_db}} provided in the GBD Benchmark Database~(GBD) by Iser~et~al.~\cite{IserS18}.
They comprise instance size features and node distribution statistics for several graph representations of SAT instances, among others, and are primarily inspired by the SATzilla~2012 features~\cite{features}.
All features are numeric and free of missing values.
We drop 10 out of 56 features because of zero variance.
Overall, prediction models have access to 46 instances features and 27 runtime features, i.e., excluding the current new solver~ $\hat{a}$.
Additionally, we retrieve instance-family information\footnote{\url{https://gbd.iti.kit.edu/getdatabase/meta_db}} to evaluate the composition of our sampled benchmarks.
Instance families comprise instances derived from the same application domain, e.g., planning, cryptography, etc., and are a valuable tool for analyzing solver performance and portfolios.

For hyper-parameter tuning, we randomly sample \SI{10}{\%} of the complete set of 5355 instances with stratification regarding the instance's family.
All instance families that are too \emph{small}, i.e., \SI{10}{\%} of them corresponds to less than one instance, are put into one meta-family for stratification.
This \emph{tuning dataset} allows for a more extensive exploration of hyper-parameter space.

\subsection{Hyper-parameters}
\label{sec:exdesign:hyper}

Given Algorithm~\ref{algALBenchmark}, there are several possible instantiations for the three phases, i.e., \emph{ranking}, \emph{selection}, and \emph{stopping}.
Also, there are different choices for the runtime-label prediction model and runtime discretization.
The experiment configurations are given below.

\subsubsection{Ranking.}

Regarding \emph{ranking} (cf. Section~\ref{sec:main:model}), we experiment with the following approaches, including our used hyper-parameter values:

\begin{itemize}
  \item Observed PAR-2 ranking of already sampled instances
  \item Predicted runtime-label ranking
  \begin{itemize}
    \item
    History size: consider the latest 1, 10, 20, 30, or 40 predictions within a voting approach for stability.
    The latest $x$ predictions for each instance vote on the instance's winning label.
    \item
    Fallback threshold: if the difference of scores between the new solver $\hat{a}$ and another solver drops below \SI{0.01}, \SI{0.05}, or \SI{0.1}, use the partially observed PAR-2 ranking as a tie-breaker.
  \end{itemize}
\end{itemize}

\subsubsection{Selection.}

For \emph{selection} (cf. Section~\ref{sec:main:selection}), we experiment with the following methods, including our used hyper-parameter values.
Since the potential runtime of experiments is by magnitudes larger than the model's update time, we only consider incrementing our benchmark by one instance at a time rather than using batches, which is also proposed in current active-learning advances~\cite{SinhaED19,2019gaal}.
A drawback of this is the lack of parallel execution of runtime experiment.

\begin{itemize}
  \item Random sampling 
  \item Uncertainty sampling
  \begin{itemize}
    \item Fallback threshold: Use random sampling for the first \SI{0}{\%}, \SI{5}{\%}, \SI{10}{\%}, \SI{15}{\%}, or \SI{20}{\%} of instances to explore the instance space.
    \item Runtime scaling: Whether to normalize uncertainty scores per instance by the average runtime of solvers on it or use the absolute values.
  \end{itemize}

  \item Information-gain sampling
  \begin{itemize}
    \item Fallback threshold: Use random sampling for the first \SI{0}{\%}, \SI{5}{\%}, \SI{10}{\%}, \SI{15}{\%}, or \SI{20}{\%} of instances to explore the instance space.
    \item Runtime scaling: Whether to normalize information-gain scores per instance by the average runtime of solvers on it or use the absolute values.
  \end{itemize}
\end{itemize}

\subsubsection{Stopping.}

For \emph{stopping} decisions (cf. Section~\ref{sec:main:stopping}), we experiment with the following criteria, including our used hyper-parameter values:

\begin{itemize}
  \item Subset-size stopping criterion, using \SI{10}{\%} or \SI{20}{\%} of instances
  \item Ranking stopping criterion
  \begin{itemize}
    \item Minimum amount: Sample at least \SI{2}{\%}, \SI{8}{\%}, \SI{10}{\%}, or \SI{12}{\%} of instances before applying the criterion.
    \item Convergence duration: Stop if the predicted ranking stays the same for a number of sampled instances equal to \SI{1}{\%} or \SI{2}{\%} of all instances.
  \end{itemize}

  \item Wilcoxon stopping criterion
  \begin{itemize}
    \item Minimum amount: Sample at least \SI{2}{\%}, \SI{8}{\%}, \SI{10}{\%}, or \SI{12}{\%} of instances before applying the criterion.
    \item Average of $p$-values to drop below: \SI{5}{\%}.
    \item Exponential-moving average: Incorporate previous significance values by using an EMA with $\beta = 0.1$ or $\beta = 0.7$.
  \end{itemize}
\end{itemize}

\subsubsection{Prediction model.}

We only use one model configuration for runtime-label prediction in our experiments since an exhaustive grid search would be infeasible.
In preliminary experiments, we compared various model types from \emph{scikit-learn}~\cite{scikit-learn}.
In particular, we conducted nested cross-validation, including hyper-parameter tuning, and used Matthews Correlation Coefficient~\cite{gorodkin2004comparing,matthews1975comparison} to assess the performance for predicting runtime labels.
Our final choice is a stacking ensemble of a quadratic-discriminant analysis and a random forest, using a decision tree to combine its ensemble members.

\subsubsection{Runtime discretization.}

To define prediction targets, i.e., discretized runtime labels, we use hierarchical clustering with $k = 3$ and a log-single-link criterion, which produced the most \emph{useful} labels in preliminary experiments.
We denote this adapted solver scoring function with $s_3$.
Other clustering approaches that we have tried include hierarchical clustering with mean-, median- and complete-link criteria, as well as $k$-means and spectral clustering.
In our chosen hierarchical procedure, each non-time-out runtime starts in a separate interval.
We then gradually merge intervals whose single-link logarithmic distance is the smallest until the desired number of partitions is reached.

To obtain \emph{useful} labels, we need to ensure that discretized labels still discriminate solvers and align with the actual PAR-2 ranking.
We analyzed the ranking induced by $s_3$ in preliminary experiments with the SAT Competition~2022 Anniversary Track~\cite{sat2022}.
According to a Wilcoxon-signed-rank test with $\alpha = 0.05$, \SI{87.83}{\%} of solver pairs have significantly different scores after discretization, only a slight drop compared to \SI{89.95}{\%} before discretization.
Further, our ranking approach correctly decides for almost all (about \SI{97.45}{\%}; $\sigma = \SI{3.68}{\%}$) solver pairs which solver is faster.
In particular, the Spearman correlation of $s_3$ and PAR-2 ranking is about \SI{0.988}{}, which is very close to the optimal value of 1~\cite{de2016comparing}.
All these results show that discretized runtimes are suitable for our framework.

\subsection{Implementation Details}

% TODO: insert URLs
For reproducibility, our source code and data are available on GitHub\footnote{temporary, anonymized version for review: \url{xxx}}.
Our code is implemented in \textsc{Python} using \emph{scikit-learn}~\cite{scikit-learn} for making predictions and \emph{gbd-tools}~\cite{IserS18} for SAT-instance retrieval.


\section{Evaluation}
\label{sec:eval}

In this section, we evaluate our active-learning framework.
First, we analyze and tune the different components of our framework on the tuning dataset.
Next, we evaluate the best configurations with the full dataset.
Finally, we analyze the importance of different instance families to our framework.

\subsection{Hyper-Parameter Analysis}

\begin{figure}[tbp!]
  \centering
  \begin{subfigure}{1.0\textwidth}
    \resizebox{\textwidth}{!}{
      \graphicspath{{../plots/}}
      \input{../plots/anni_train_color_ranking.pgf}
    }
    \caption{Ranking approaches}
    \label{fig:annitraincolorranking}
  \end{subfigure}
  \\
  \vspace{0.2cm}
  \begin{subfigure}{1.0\textwidth}
    \resizebox{\textwidth}{!}{
      \graphicspath{{../plots/}}
      \input{../plots/anni_train_color_selection.pgf}
    }
    \caption{Selection approaches}
    \label{fig:annitraincolorselection}
  \end{subfigure}
  \\
  \vspace{0.2cm}
  \begin{subfigure}{1.0\textwidth}
    \resizebox{\textwidth}{!}{
      \graphicspath{{../plots/}}
      \input{../plots/anni_train_color_stopping.pgf}
    }
    \caption{Stopping criteria}
    \label{fig:annitraincolorstopping}
  \end{subfigure}
  \caption{
    $O_{\operatorname{rt}}$-$O_{\operatorname{acc}}$-diagrams comparing different hyper-parameter instantiations of our active-learning framework.
    The x-axis shows the ratio of total solver runtime on the sampled instances relative to all instances.
    The y-axis shows the ranking accuracy (cf. Section~\ref{sec:exdesign:eval}).
    Each line entails the front of Pareto-optimal configurations for the respective hyper-parameter instantiation.
  }
  \label{fig:e2eallsolvers}
\end{figure}

Our experiments follow the evaluation framework introduced in Section~\ref{sec:exdesign:eval}.
Fig.~\ref{fig:e2eallsolvers} shows the performance of the approaches from Section~\ref{sec:exdesign:hyper} on $O_{\operatorname{rt}}$-$O_{\operatorname{acc}}$-diagrams for the hyper-parameter-tuning dataset.
Evaluating a particular configuration with Algorithm~\ref{alg:eval} returns a point $\left(O_{\operatorname{rt}},\, O_{\operatorname{acc}}\right)$.
We do not show intermediate results of the active-learning procedure but only the final results after stopping.
The plotted lines represent the best performing configurations (convex hull) per top-level hyper-parameter choice, i.e., per ranking approach (Fig.~\ref{fig:annitraincolorranking}), selection approach (Fig.~\ref{fig:annitraincolorselection}), and stopping criterion (Fig.~\ref{fig:annitraincolorstopping}).

Regarding ranking approaches (Fig.~\ref{fig:annitraincolorranking}), using the $s_3$-induced runtime-label ranking consistently outperforms the partially observed PAR-2 ranking for each possible value of the trade-off parameter~$\delta$.
This outcome is expected since selection decisions are not random.
For example, we might sample more instances of one family if it benefits discrimination of solvers.
While the partially observed PAR-2 score is skewed, the prediction model can account for this.

Regarding the selection approaches (Fig.~\ref{fig:annitraincolorselection}), uncertainty sampling performs best in most cases.
However, information-gain sampling is beneficial if runtime is strongly favored (small $\delta$; runtime fraction less than \SI{5}{\%}).
This result aligns with our expectations:
Information-gain sampling selects instances that maximize the expected reduction in entropy.
This means we sample instances revealing similarities between solvers rather than differences, which helps to build a confident model fast.
However, the method lacks the possibility of selecting helpful instances for distinguishing solvers later.
Random sampling performs reasonably well but is outperformed by uncertainty sampling in all cases, showing the benefit of actively selecting instances based on a prediction model.

Regarding the stopping criterion (Fig.~\ref{fig:annitraincolorstopping}), the ranking stopping criterion performs the most consistently well.
If accuracy is strongly favored (very high $\delta$), the Wilcoxon stopping criterion performs better.
The subset-size stopping criterion performs reasonably well but does not improve beyond a certain accuracy because of sampling a fixed subset of instances.

\begin{figure}[tb!]
  \centering
  \begin{subfigure}{0.4775\textwidth}
    \resizebox{\textwidth}{!}{
      \graphicspath{{../plots/}}
      \input{../plots/anni_train_optimization_goal.pgf}
    }
    \caption{Runtime vs. Instances}
    \label{fig:annitrainoptgoalruntime}
  \end{subfigure}
  \begin{subfigure}{0.5125\textwidth}
    \resizebox{\textwidth}{!}{
      \graphicspath{{../plots/}}
      \input{../plots/anni_train_delta_acc.pgf}
    }
    \caption{Runtime vs. Accuracy}
    \label{fig:annitrainoptgoalacc}
  \end{subfigure}

  \caption{
    Scatter plot comparing different instantiations of $\delta$ for our active-learning strategy on the hyper-parameter-tuning dataset.
    The x-axis shows the fraction of runtime $O_{\operatorname{rt}}$ of the sample, while the y-axes show the fraction of instances sampled and ranking accuracy, respectively.
    The color indicates the weighting between different optimization goals $\delta \in \left[0, 1\right]$.
    The larger $\delta$, the more we favor accuracy over runtime.
  }
  \label{fig:annitrainoptgoal}
\end{figure}

Figure~\ref{fig:annitrainoptgoalruntime} shows an interesting consequence of weighting our optimization goals:
If we, on the one hand, desire to get a \emph{rough} estimate of a solver's performance fast (low $\delta$), approaches favor selecting many \emph{easy} instances.
In particular, the fraction of sampled instances is larger than the fraction of runtime.
By having many observations, it is easier to build a model.
If we, on the other hand, desire to get a \emph{good} estimate of a solver's performance in a moderate amount of time (high $\delta$), approaches favor selecting fewer \emph{difficult} instances.
In particular, the fraction of instances is smaller than the fraction of runtime.

Furthermore, Figure~\ref{fig:annitrainoptgoalacc} reveals which values make the most sense for $\delta$.
The range $\delta \in \left[0.2, 0.8\right]$, thereby, corresponds to the points with a runtime fraction between \SI{0.03} and \SI{0.22}.
We consider this region to be most promising, analogous to the \emph{elbow} method in cluster analysis~\cite{kodinariya2013review}.

\subsection{Full-Dataset Evaluation}

Having selected the most promising hyper-parameters, we run our active-learning experiments on the complete Anniversary Track dataset (5355 instances).
The aforementioned range $\delta \in \left[0.2, 0.8\right]$ is covered by only two configurations.
The best-performing approach for $\delta \in \left[0.2, 0.7\right]$ uses the predicted runtime-label ranking, information-gain sampling, and ranking stopping criterion.
It can predict a new solver's PAR-2 ranking with about \SI{90.48}{\%} accuracy ($O_{\operatorname{acc}}$), while only needing \SI{5.41}{\%} of the full evaluation time ($O_{\operatorname{rt}}$).
The best-performing approach for $\delta \in (0.7, 0.8]$ uses the predicted runtime-label ranking, uncertainty sampling, and ranking stopping criterion.
It can predict a new solver's PAR-2 ranking with about \SI{92.33}{\%} accuracy ($O_{\operatorname{acc}}$), while only needing \SI{10.35}{\%} of the full evaluation time ($O_{\operatorname{rt}}$).

\begin{table}[t]
  \centering
  \caption{
      Performance comparison of the best-performing active-learning approaches (\emph{AL}), random sampling of the same runtime fraction with 1000 repetitions (\emph{Random}) and statically selecting the instances that are most frequently sampled by active-learning approaches (\emph{Most Freq.})
    }
  \label{tab:fulldataset}

  \begin{subfigure}{\textwidth}
    \centering
    \caption{Best-performing AL approach for $\delta \in \left[0.2, 0.7\right]$}
    \begin{tabular}{
      >{\arraybackslash}p{0.4\textwidth}
      S[table-format=2.2,table-column-width=0.15\textwidth]
      S[table-format=2.2,table-column-width=0.15\textwidth]
      S[table-format=2.2,table-column-width=0.15\textwidth]
    }
      \hline
        & {AL} & {Random} & {Most Freq.} \\
      \hline
      Sampled Runtime Fraction (\%) & 5.41 & 5.43 & 5.44 \\
      Sampled Instance Fraction (\%) & 26.53 & 5.43 & 27.75 \\
      Ranking Accuracy (\%) & 90.48 & 88.54 & 81.08 \\
      \hline
    \end{tabular}
  \end{subfigure}
  \\
  \begin{subfigure}{\textwidth}
    \centering
    \caption{Best-performing AL approach for $\delta \in (0.7, 0.8]$}
    \begin{tabular}{
      >{\arraybackslash}p{0.4\textwidth}
      S[table-format=2.2,table-column-width=0.15\textwidth]
      S[table-format=2.2,table-column-width=0.15\textwidth]
      S[table-format=2.2,table-column-width=0.15\textwidth]
    }
      \hline
        & {AL} & {Random} & {Most Freq.} \\
      \hline
      Sampled Runtime Fraction (\%) & 10.35 & 10.37 & 10.37 \\
      Sampled Instance Fraction (\%) & 5.24 & 10.37 & 36.96 \\
      Ranking Accuracy (\%) & 92.33 & 91.61 & 84.52 \\
      \hline
    \end{tabular}
  \end{subfigure}
\end{table}

Table~\ref{tab:fulldataset} shows how both active-learning approaches compare against static baselines.
The first column shows the two aforementioned AL configurations.
We compare it against a random baseline and a static benchmark set consisting of the most frequently sampled instances by active learning.
The random baseline (\emph{random}) samples instances until roughly the same fraction of runtime is in the benchmark set.
We repeat this 1000 times and report average results.
The benchmark set of the most frequently AL-sampled instances (\emph{Most Freq.}) uses the average sampling frequencies over all solvers using all Pareto-optimal active-learning approaches.

Our AL approaches perform slightly better compared to random sampling.
However, differences are not significant regarding a Wilcoxon signed-rank test with $\alpha = 0.05$.
Note that the performance difference between random and non-random sampling changes for different fractions of sampled runtime (cf. Figure~\ref{fig:annitraincolorselection}).
The static benchmark using the most frequently AL-sampled instances performs poorly, though, in comparison to active learning and random sampling.
This is somewhat expected since it does not reflect the right balance of instance families:
Families whose instances are uniform-randomly selected by AL appear less often in this benchmark than families where some instances are sampled more often than others.
While the active-learning results are less strong on the full dataset than on the smaller tuning dataset, it still shows the benefit of making benchmark selection dependent on the solvers to distinguish.

\subsection{Instance-Family Importance}

\begin{figure}[tb]
  \centering
  \resizebox{0.85\textwidth}{!}{
    \graphicspath{{../plots/}}
    \input{../plots/anni_final_families.pgf}
  }
  \caption{
    Scatter plot showing the \emph{importance} of different instance families to our framework.
    The x-axis shows the frequency of instance families in the full Anniversary Track dataset.
    The y-axis shows the average frequency of instance families in the samples selected by active learning.
    The dashed line represents families that occur with the same frequency in both, the dataset and samples.
  }
  \label{fig:annifinalfamilies}
\end{figure}

Selection decisions of our approach also reveal the importance of different instance families to our framework.
Figure~\ref{fig:annifinalfamilies} shows the occurrence of instance families within the dataset and the benchmarks created by active learning.
We use the best-performing configurations for all $\delta \in \left[0, 1\right]$ and examine the selection decisions by the active-learning approach on the SAT Competition~2022 Anniversary Track dataset~\cite{sat2022}.
While most families appear with the same fraction in the dataset and the sampled benchmarks, a few outliers need further discussion.
Problem instances of the families \emph{fpga}, \emph{quasigroup-completion}, and \emph{planning} are especially helpful to our framework in distinguishing solvers.
Instances of these families are selected over-proportionally in comparison to the full dataset.
In contrast, instances of the largest family, i.e., \emph{hardware-verification}, roughly appear with the same fraction in the dataset and the sampled benchmarks.
Finally, instances of the family \emph{cryptography} are less important in distinguishing solvers than their vast weight in the dataset suggests.
A possible explanation is that these instances are very similar, such that a small fraction of them is sufficient to estimate a solver's performance on all of them.

\section{Conclusion}

In this work, we have discussed possible solutions to the \emph{New-Solver Problem}:
Given a new solver, we want to find its ranking amidst its competitors.
Our approach provides accurate ranking predictions while needing significantly less runtime than a full evaluation on a given benchmark set.
In particular, we can determine a new solver's PAR-2 ranking with about \SI{92}{\%} accuracy while only needing \SI{10}{\%} of the full-evaluation time.
Our framework produces good results working with discrete runtime labels rather than regressing actual runtimes.
We have evaluated several ranking algorithms, instance-selection approaches, and stopping criteria within our sequential active-learning process.
We also took a brief look at which instance families are the most prevalent in selection decisions.

\subsection{Future Work}

Future work may compare further ranking algorithms, instance-selection approaches, and stopping criteria.
Furthermore, it is possible to formulate runtime discretization as an optimization problem, directly selecting the most discriminatory discretization technique rather than making an empirical comparison.

A major shortcoming of our current approach is the lack of parallelization, selecting instances one at a time.
Benchmarking on a computing cluster with $n$ cores benefits from having batches of $n$ instances.
However, bigger batch sizes $n$ impede \emph{active learning}.
Also, it is unclear how to synchronize the model update and instance selection without wasting too much runtime.

On a more general note, one can apply our evaluation framework to arbitrary $\mathcal{NP}$-complete problems, as all discussed active-learning methods are problem-agnostic.
Especially the runtime-prediction model would need some tweaking for this purpose, though.
Those problems share most of the relevant properties of SAT solving, i.e., there are established instance features, a complete benchmark run takes quite some time, and solver development traditionally requires expert knowledge to hand-select instances.


%
% ---- Bibliography ----
%
\bibliographystyle{splncs04}
\bibliography{literature}

\end{document}
