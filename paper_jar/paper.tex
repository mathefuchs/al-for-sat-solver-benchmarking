\documentclass[sn-basic, Numbered]{sn-jnl} % may use "lineno" for line numbers
% template from https://www.springernature.com/gp/authors/campaigns/latex-author-support

\usepackage[ruled,vlined,linesnumbered,norelsize]{algorithm2e}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{siunitx}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage[table]{xcolor}
\usepackage{tikz} % has to be loaded after "xcolor" here, else "option clash"

\newcommand{\cmark}{\ding{51}} % symbols for table comparing benchmarking approaches
\newcommand{\xmark}{\ding{55}}

\DontPrintSemicolon % configuration for algorithm2e
\def\NlSty#1{\textnormal{\fontsize{8}{10}\selectfont{}#1}}
\SetKwSty{texttt}
\SetCommentSty{emph}

\newtheorem{definition}{Definition} % only kind of theorems we use are definitions (well, one)

\raggedbottom

\begin{document}

\title[Active Learning for SAT Solver Benchmarking]{Active Learning for SAT Solver Benchmarking {\large (extended and revised version)}} % short and long title

\author*[1]{\fnm{Tobias} \sur{Fuchs}}\email{tobias.fuchs@kit.edu}
\author[1]{\fnm{Jakob} \sur{Bach}}\email{jakob.bach@kit.edu}
\author[1]{\fnm{Markus} \sur{Iser}}\email{markus.iser@kit.edu}

\affil[1]{\orgname{Karlsruhe Institute of Technology (KIT)}, \orgaddress{\city{Karlsruhe}, \country{Germany}}}

\abstract{
Benchmarking is crucial for developing new algorithms.
This also applies to solvers for the propositional satisfiability (SAT) problem.
Benchmark selection is about choosing representative problem instances that reliably discriminate solvers based on their runtime.
In this paper, we present a dynamic benchmark selection approach based on active learning.
Our approach estimates the rank of a new solver among its competitors, striving to minimize benchmarking runtime but maximize ranking accuracy.
Instead of using real-valued solver runtimes, our approach works with discretized runtime labels, which yielded better solver rank predictions.
We evaluated this approach on the Anniversary Track dataset from the SAT Competition 2022.
Our benchmark selection approach can predict the rank of a new solver after approximately \SI{10}{\%} of the time it would take to run the solver on all instances of this dataset, with a prediction accuracy of approximately \SI{92}{\%}. 
Additionally, we discuss the importance of instance families in the selection process.
In conclusion, our tool offers a reliable method for solver engineers to assess a new solver's performance efficiently.
}

\keywords{Propositional satisfiability, Benchmark selection, Active learning}

\maketitle

\section{Introduction}
\label{sec:intro}

One of the main phases of algorithm engineering is benchmarking.
This also applies to solvers for propositional satisfiability (SAT), the canonical $\mathcal{NP}$-complete problem.
Benchmarking is, however, quite expensive regarding the runtime of experiments.
While it is still feasible to benchmark a single or a small number of given SAT solvers, developing new, competitive SAT solvers requires extensive experimentation with a variety of ideas.
The latter often results in a combinatorial explosion of the configuration space~\cite{HutterHL11}.
In particular, a new solver idea is rarely best on the first try.
Thus, it is highly desirable to reduce benchmarking time and discard unpromising ideas early, allowing to test more approaches or spend more time on promising ones.
The field of SAT solver benchmarking is well established, yet traditional benchmark selection approaches do not optimize benchmark runtime.
Instead, they focus on selecting a representative set of instances for ranking solvers~\cite{Gelder11,HoosKSS13}.
In this regard, SAT Competitions typically employ the \mbox{PAR-2} score, which, given a time limit of $\tau$, is the average solver runtime with a penalty of $2 \tau$ for timeouts~\cite{FroleyksHIJS21}.

\subparagraph{Problem statement}

In this paper, we present a novel benchmark selection approach based on active learning.
Our approach can predict the rank of a new solver with high accuracy in only a fraction of the time needed to evaluate the complete benchmark.
Definition~\ref{def:new-solver-problem} specifies the problem we address.
Note that our scenario assumes knowing the runtimes of all solvers, except the new one, on all instances.
One could also imagine a collaborative filtering scenario, where runtimes are only partially known~\cite{misir2017data,misir2017alors}.

\begin{definition}[New-Solver Problem]
  Let us consider a set of solvers~$\mathcal{A}$, instances~$\mathcal{I}$, and runtimes~${r\!: \mathcal{A} \times \mathcal{I} \rightarrow \left[0, \tau\right]}$, with each solver having a runtime limit of $\tau$.
  The \emph{New-Solver Problem} is about selecting benchmark instances from $\mathcal{I}$ to maximize the confidence in predicting the rank of a new solver $\hat{a} \notin \mathcal{A}$, while minimizing the summed runtime of~$\hat{a}$ over all selected instances.
  \label{def:new-solver-problem}
\end{definition}

The two objectives of this problem are conflicting, i.e., the ranking's confidence typically benefits from more instances while the runtime criterion encourages less or at least shorter evaluations.
The crux is finding a good trade-off.

The approach presented in this paper meets several criteria for benchmarking that are considered desirable (cf.~Table~\ref{tab:requirements}).
Rather than outputting a binary classification, namely whether the new solver is worse than an existing solver or not, we provide a \emph{scoring} function that shows by which margin a solver is worse and how similar its performance is to existing solvers.
This helps to identify the solvers that have the most similar runtimes more quickly.
In particular, our approach enables \emph{ranking} the new solver among a set of existing solvers in one benchmarking run.
We show that predicting the exact per-instance runtime of the solvers, which would be a more challenging task, is not necessary for ranking.
Instead, we work with discretized runtime labels.
We use instance features and known solver runtimes to predict these labels.
Furthermore, we minimize the \emph{runtime} required for our approach to arrive at its conclusion.
Moreover, we select benchmark instances \emph{non-randomly} and \emph{incrementally}.
In particular, we consider runtime information from experiments already done when choosing the next instance.
By doing so, we can control the properties of the benchmarking approach, such as its required runtime.
Our approach is \emph{scalable} in that it ranks a new solver $\hat{a}$ among any number of known solvers $\mathcal{A}$.
In particular, we only subsample the benchmark once instead of comparing pairwise against each other solver~\cite{MatriconAFSH21}.

\begin{table}[tbp]
  \centering
  \caption{Comparison of features of our benchmark-selection approach, the static benchmark-selection approach by Hoos~et~al.~\cite{HoosKSS13}, the algorithm configuration system SMAC~\cite{HutterHL11}, and the active-learning approaches by Matricon et al.~\cite{MatriconAFSH21}.
  }
  \label{tab:requirements}
  \begin{tabular}{
    m{0.2752\textwidth}
    >{\centering\arraybackslash}m{0.1376\textwidth}
    >{\centering\arraybackslash}m{0.1376\textwidth}
    >{\centering\arraybackslash}m{0.1376\textwidth}
    >{\centering\arraybackslash}m{0.1376\textwidth}
  }
    \toprule
    Feature & Hoos~\cite{HoosKSS13} & SMAC~\cite{HutterHL11} & Matricon~\cite{MatriconAFSH21} & Our approach \\
    \midrule
    Ranking/Scoring & \cmark & \xmark & (\cmark) & \cmark \\
    Runtime Minimization & \xmark & \cmark & \cmark & \cmark \\
    Incremental/Non-Random & \xmark & \xmark & \cmark & \cmark \\
    Scalability & \cmark & \cmark & \xmark & \cmark \\
    \bottomrule
  \end{tabular}
\end{table}

\subparagraph{Experiments}

We evaluated our approach using the SAT Competition~2022 Anniversary Track dataset~\cite{sat2022}, consisting of 5355~instances and runtimes of 28~solvers.
Cross-validation was performed by treating each solver as the new solver once and learning to predict its PAR-2 rank.
On average, our predictions achieve a ranking accuracy of approximately \SI{92}{\%} with only approximately \SI{10}{\%} of the runtime required to evaluate these solvers on the complete set of instances.
Our entire source code\footnote{\url{https://github.com/mathefuchs/al-for-sat-solver-benchmarking}} and experimental data\footnote{\url{https://github.com/mathefuchs/al-for-sat-solver-benchmarking-data}} are available on GitHub.

\subparagraph{Outline}

The remainder of this paper is organized as follows:
Section~\ref{sec:related} reviews related work.
Section~\ref{sec:main} describes our active learning approach.
Section~\ref{sec:exdesign} introduces the experimental design.
Section~\ref{sec:eval} evaluates the experimental results.
Section~\ref{sec:conclusion} concludes.

\subparagraph{Disclaimer}

This article is an extended and refined version of our conference paper titled ``Active Learning for SAT Solver Benchmarking''~\cite{fuchs2023active}.
In particular, we significantly extended the analysis of runtime-prediction and runtime-discretization approaches (cf.~Section~\ref{sec:exdesign:disc-pred}), which determined our chosen solver runtime model for instance selection.
Furthermore, we expanded the evaluation of which benchmark instances are selected by our approach (cf.~Section~\ref{sec:eval:instance}).

\section{Related Work}
\label{sec:related}

Benchmarking is of great interest in numerous research areas and represents an active field of research in its own right.
Studies have shown that the compilation of benchmark instances poses several challenges.
Using biased benchmarks can easily lead to fallacious interpretations~\cite{abs-2107-07002}.
Benchmarking also involves several interchangeable elements, including the performance measures used, how the measures are aggregated, and how missing values are handled.
Questionable research practices could alter these elements a-posteriori to meet expectations, thereby skewing the results~\cite{NiesslHWCB22}.

The following sections discuss related work from the areas of static benchmark selection (cf.~Section~\ref{sec:related:static-selection}), algorithm configuration (cf.~Section~\ref{sec:related:algo-configuration}), incremental benchmark selection (cf.~Section~\ref{sec:related:incremental-selection}), and active learning (cf.~Section~\ref{sec:related:active-learning}).
Table~\ref{tab:requirements} compares the most relevant approaches, which all pursue slightly different goals.
In particular, our approach is \emph{not} a general improvement over the others but the only one fully aligned with Definition~\ref{def:new-solver-problem}.

\subsection{Static Benchmark Selection}
\label{sec:related:static-selection}

Benchmark selection is essential for solver competitions like the SAT Competition.
In such competitions, the organizers define the rules for composing the benchmarks.
These selection strategies are primarily static since they do not depend on particular solvers to distinguish.
Balint et al. provide an overview of benchmark-selection criteria in different solver competitions~\cite{balint2015overview}.
Froleyks et al. describe benchmark selection in recent SAT~competitions~\cite{FroleyksHIJS21}.
Manthey and Möhle propose an approach to remove redundancy from competition benchmarks by considering feature equivalence of formulas~\cite{manthey2016better}.
M{\i}s{\i}r also presents a feature-based approach to reduce benchmarks by using matrix factorization and clustering~\cite{misir2021benchmark}.

Hoos et al.~\cite{HoosKSS13} discuss desirable properties of SAT benchmark instances.
They identify three key selection criteria: instance variety to avoid over-fitting, adapted instance hardness (neither too easy nor too hard), and avoiding duplicate instances. 
To filter instances that are too similar, they employ a distance-based approach with the SATzilla features~\cite{XuHHL08,features}.
It should be noted, however, that the approach does not optimize for benchmark \emph{runtime}.
Instead, instances are selected \emph{at random} except considering constraints on instance hardness and feature distance.

\subsection{Algorithm Configuration}
\label{sec:related:algo-configuration}

Further related work can be found within the field of algorithm configuration~\cite{HoosHL21,Stutzle0P22}, as exemplified by the configuration system SMAC~\cite{HutterHL11}.
Thereby, the goal is to tune SAT solvers for a given sub-domain of problem instances.
While this task differs from our goal, for instance, in that we do not need to navigate the configuration space, there are similarities to our approach as well.
For example, SMAC employs an iterative, model-based selection procedure similar to our approach, but this is for selecting configurations rather than instances.
In contrast, instance selection is made \emph{randomly} in SMAC, i.e., without building a model over instances.
Furthermore, since algorithm configuration is designed to identify the optimal configuration, an algorithm configurator cannot be used to \emph{rank} or \emph{score} a new solver relative to others.

\subsection{Incremental Benchmark Selection}
\label{sec:related:incremental-selection}

Matricon~et~al. present an incremental benchmark selection approach~\cite{MatriconAFSH21}.
Their \emph{per-set efficient algorithm selection problem} (PSEAS) is similar to our \emph{New-Solver Problem}, as given in Definition~\ref{def:new-solver-problem}.
To compare \emph{a pair of} SAT solvers, the authors propose an iterative approach to select a solver-specific subset of instances until a desired confidence level is reached.
This is achieved by calculating a scoring metric for all unselected instances, running the experiment with the highest score, and updating the confidence.
Their approach ticks off most of the desired features in Table~\ref{tab:requirements}.
However, the approach only compares solvers pairwise rather than providing a multi-solver \emph{scoring} or \emph{ranking}.
Consequently, it is unclear how similar two given solvers are or on which instances they behave similarly.
Furthermore, a significant shortcoming is the lack of \emph{scalability} with the number of solvers.
Due to comparing only pairs of solvers, evaluating a new solver with their method requires sampling an individual set of benchmark instances for each existing solver.
In particular, comparing against one existing solver will typically require a different set of instances than comparing against another one.
In contrast, our approach allows comparing a new solver against a set of existing solvers by sampling only one set of benchmark instances.

\subsection{Active Learning}
\label{sec:related:active-learning}


\begin{figure}[tbp!]
  \centering
  \begin{tabular}[c]{ccc}
  \begin{subfigure}[b]{0.24\textwidth}
  \centering
  \resizebox{!}{2.75cm}{
  \begin{tikzpicture}
  \draw[fill=black]  (-2.8,3.7) ellipse (0.1 and 0.1);
  \draw[fill=black]  (-2.6,3.4) arc (0:180:0.2);
  \node[right] at (-5.9,3.6) {Agent (Solver)};
  \draw[fill=white]  (-3.3,2.8) rectangle (-2.5,2.2);
  \draw[fill=white]  (-3.2,2.7) rectangle (-2.4,2.1);
  \draw[fill=white]  (-3.1,2.6) rectangle (-2.3,2);
  \node[right] at (-5.9,2.4) {Training Data};
  \node (v1) at (-2.8,3.4) {};
  \node (v2) at (-2.8,2.8) {};
  \draw[->]  (v1) edge (v2);
  \node[right] at (-2.8,3.1) {$x, y$};
  \node (v3) at (-2.8,2) {};
  \node (v4) at (-2.8,1.4) {};
  \draw[->]  (v3) edge (v4);
  \draw  (-3.1,1.4) rectangle (-2.5,1);
  \draw  (-2.4,3.2) rectangle (-2.4,3.2);
  \node[right] at (-5.9,1.2) {Learning Alg.};
  \node (v5) at (-2.8,1) {};
  \node (v6) at (-2.8,0.4) {};
  \draw[->]  (v5) edge (v6);
  \node[right] at (-5.9,0.25) {Predic. Function};
  \node at (-2.8,0.25) {$f$};
  \end{tikzpicture}
  }
  \caption{Passive.}
  \label{fig:passive}
  \end{subfigure}
  &
  \begin{subfigure}[b]{0.33\textwidth}
  \centering
  \resizebox{!}{2.75cm}{
  \begin{tikzpicture}
  \draw[fill=black]  (-2.8,3.7) ellipse (0.1 and 0.1);
  \draw[fill=black]  (-2.6,3.4) arc (0:180:0.2);
  \node[right] at (-5.9,3.6) {Agent (Solver)};
  \draw[fill=white]  (-3.3,2.8) rectangle (-2.5,2.2);
  \draw[fill=white]  (-3.2,2.7) rectangle (-2.4,2.1);
  \draw[fill=white]  (-3.1,2.6) rectangle (-2.3,2);
  \node[right] at (-5.9,2.4) {Training Data};
  \node (v1) at (-2.8,3.4) {};
  \node (v2) at (-2.8,2.8) {};
  \draw[->]  (v1) edge (v2);
  \node[right] at (-2.8,3.1) {$x, y$};
  \node (v3) at (-2.8,2) {};
  \node (v4) at (-2.8,1.4) {};
  \draw[->]  (v3) edge (v4);
  \draw  (-3.1,1.4) rectangle (-2.5,1);
  \draw  (-2.4,3.2) rectangle (-2.4,3.2);
  \node[right] at (-5.9,1.2) {Learning Alg.};
  \node (v5) at (-2.8,1) {};
  \node (v6) at (-2.8,0.4) {};
  \draw[->]  (v5) edge (v6);
  \node[right] at (-5.9,0.25) {Predic. Function};
  \node at (-2.8,0.25) {$f$};
  \node[right] at (-1.2,2.4) {Pool};
  \draw[->, densely dashed] (-2.4,1.2) arc (-90:0:0.8);
  \draw[->, densely dashed] (-1.6,2.8) arc (0:90:0.8);
  \node[right] at (-1.6,3.135) {$x, ?$};
  \draw  (-1.6,2.4) ellipse (0.4 and 0.3);
  \draw[fill=black]  (-1.725,2.5) ellipse (0.07 and 0.07);
  \draw[fill=black]  (-1.625,2.25) ellipse (0.07 and 0.07);
  \draw[fill=black]  (-1.425,2.4) ellipse (0.07 and 0.07);
  \end{tikzpicture}
  }
  \caption{Active Pool-based.}
  \label{fig:activepool}
  \end{subfigure}
  &
  \begin{subfigure}[b]{0.33\textwidth}
  \centering
  \resizebox{!}{2.75cm}{
  \begin{tikzpicture}
  \draw[fill=black]  (-2.8,3.7) ellipse (0.1 and 0.1);
  \draw[fill=black]  (-2.6,3.4) arc (0:180:0.2);
  \node[right] at (-5.9,3.6) {Agent (Solver)};
  \draw[fill=white]  (-3.3,2.8) rectangle (-2.5,2.2);
  \draw[fill=white]  (-3.2,2.7) rectangle (-2.4,2.1);
  \draw[fill=white]  (-3.1,2.6) rectangle (-2.3,2);
  \node[right] at (-5.9,2.4) {Training Data};
  \node (v1) at (-2.8,3.4) {};
  \node (v2) at (-2.8,2.8) {};
  \draw[->]  (v1) edge (v2);
  \node[right] at (-2.8,3.1) {$x, y$};
  \node (v3) at (-2.8,2) {};
  \node (v4) at (-2.8,1.4) {};
  \draw[->]  (v3) edge (v4);
  \draw  (-3.1,1.4) rectangle (-2.5,1);
  \draw  (-2.4,3.2) rectangle (-2.4,3.2);
  \node[right] at (-5.9,1.2) {Learning Alg.};
  \node (v5) at (-2.8,1) {};
  \node (v6) at (-2.8,0.4) {};
  \draw[->]  (v5) edge (v6);
  \node[right] at (-5.9,0.25) {Predic. Function};
  \node at (-2.8,0.25) {$f$};
  \node[right, label={[align=left]Gen.\\Model}] at (-0.75,1.88) {};
  \node[right] at (-1.3,3.135) {$x, ?$};
  \draw[->, densely dashed] (-2.4,1.2) arc (-90:90:1.2);
  \end{tikzpicture}
  }
  \caption{Active Synthesis-based.}
  \label{fig:activesynth}
  \end{subfigure}
  \end{tabular}
  \caption{Types of machine learning (depiction inspired by Rubens~et~al.~\cite{RubensESK15}).}
  \label{fig:learning}
\end{figure}

Our benchmark selection approach uses active learning.
In passive machine learning, prediction models are trained on datasets with given instance labels (cf.~Fig.~\ref{fig:passive}).
In contrast, active learning (AL) begins with little or no labeled data.
AL then repeatedly selects interesting problem instances for which to acquire labels, gradually improving the prediction model (cf.~Fig.~\ref{fig:activepool}).
AL methods are particularly advantageous when acquiring labels is computationally expensive, such as obtaining solver runtimes.
Without AL methods, it is not evident which instances to label and which not.
In our scenario, the first goal is to maximize the utility of instances to our prediction model, which is measured by the ranking accuracy.
Our second goal is to minimize the cost of label acquisition, which is estimated by the instance's predicted runtime.
Consequently, our overall objective is to develop an accurate prediction model without labeling each data point.

Rubens et~al.~\cite{RubensESK15} survey active-learning advances.
While synthesis-based AL methods~\cite{0001AEMN22,GarzonMG22,2019gaal} generate instances for labeling, pool-based methods~\cite{GolbandiKL11,HarpaleY08,KorenBV09} rely on a fixed set of unlabeled instances to sample from.
Recent synthesis-based methods within the field of SAT solving demonstrate the generation of problem instances with desired properties~\cite{0001AEMN22,GarzonMG22}.
However, this objective is distinct from ours.
While those approaches aim to generate instances on which a solver is either effective or ineffective, our objective is to predict whether a solver is effective or ineffective on an \emph{existing} benchmark.
In the latter direction, Volpato and Guangyan employ pool-based AL to train an instance-specific algorithm selector~\cite{volpato2019active}.
However, their objective is not to benchmark a solver's overall performance but to recommend the best solver for each SAT instance from a set of available solvers.


\section{Active Learning for SAT Solver Benchmarking}
\label{sec:main}

Algorithm~\ref{algALBenchmark} outlines our benchmarking framework. 
Given a set of solvers~$\mathcal{A}$, instances~$\mathcal{I}$, and runtimes~$r$, we first initialize a prediction model~$\mathcal{M}$ for the new solver $\hat a \not\in \mathcal{A}$ (Line~1).
The prediction model~$\mathcal{M}$ is used to select instances repeatedly (Line~4) for benchmarking the new solver $\hat a$ (Line~5). 
The acquired result is subsequently employed to update the prediction model (Line~7). 
When the stopping criterion is met (Line~3), the benchmarking loop terminates, and the final score of $\hat{a}$ is estimated (Line~8). 
Algorithm~\ref{algALBenchmark} returns this score, along with the acquired benchmark instances and runtime measurements. 

Section~\ref{sec:main:model} describes the underlying prediction model~$\mathcal{M}$ and outlines the methodology for deriving a solver ranking from it.
Section~\ref{sec:main:selection} presents the criteria for selecting instances.
Finally, Section~\ref{sec:main:stopping} presents potential stopping conditions.

\begin{algorithm}[t]
  \caption{Incremental Benchmarking Framework}
  \label{algALBenchmark}

  \KwIn{Solvers $\mathcal{A}$, Instances $\mathcal{I}$, Runtimes $r : \mathcal{A} \times \mathcal{I} \rightarrow [0, \tau]$, Solver $\hat{a}$}
  \KwOut{Predicted Score of $\hat{a}$, Measured Runtimes $\mathcal{R}$}

  \BlankLine

  $\mathcal{M} \leftarrow \operatorname{initModel}\left(\mathcal{A},\, \mathcal{I},\, r\right)$ \tcp*{cf. Section~\ref{sec:main:model}}
  
  \BlankLine
  $\mathcal{R} \leftarrow \emptyset$ \;
  \While(\tcp*[f]{cf. Section~\ref{sec:main:stopping}}){$\operatorname{not} \operatorname{stop}\left(\mathcal{M}\right)$}{
    $e \leftarrow \operatorname{selectNextInstance}\left(\mathcal{M}\right)$ \tcp*{cf. Section~\ref{sec:main:selection}}

    $t \leftarrow \operatorname{runExperiment}\left(\hat{a},\,  e\right)$  \tcp*{Runs $\hat{a}$ on $e$ with timeout $\tau$}

    $\mathcal{R} \leftarrow \mathcal{R} \cup \left\lbrace (e,\, t) \right\rbrace$

    \BlankLine
    $\operatorname{updateModel}\left(\mathcal{M},\, \mathcal{R}\right)$ \tcp*{cf. Section~\ref{sec:main:model}}
  }
  $s_{\hat a} \leftarrow \operatorname{predictScore}(\mathcal{M})$ \tcp*{cf. Section~\ref{sec:main:model}}
  
  \BlankLine
  \Return $(s_{\hat a}, \mathcal{R})$
\end{algorithm}


\subsection{Solver Model}
\label{sec:main:model}

The model~$\mathcal{M}$ provides a prediction function $f_{\hat a} : \mathcal{I} \rightarrow \mathbb{R}$ for the new solver $\hat a$.
This prediction function powers instance selection as described in Section~\ref{sec:main:selection}.
In the update step (Algorithm~\ref{algALBenchmark},~Line~7), the model is trained to predict a transformed version of solver runtime (cf.~Section~\ref{sec:main:model:transformation}) for the solver~$\hat{a}$, using the features described in Section~\ref{sec:exdesign:data} and the previously acquired runtimes~$\mathcal{R}$.
Each iteration trains a new prediction model from scratch since the time for running the solver~$\hat{a}$ on one instance~$e$ (Line~5) dominates the training time of the model~$\mathcal{M}$ on all acquired instances~$\mathcal{R}$ by a significant margin.
Once the benchmarking loop terminates, the score of the solver~$\hat{a}$ is predicted and then returned (Lines~8--9).

\subsubsection{Runtime Transformation}
\label{sec:main:model:transformation}

For the prediction model~$\mathcal{M}$, we transform the real-valued runtimes into discrete runtime labels on a per-instance basis.
For each instance $e \in \mathcal{I}$, we use a clustering algorithm to assign the runtimes in $\bigl\{ r(a, e) \mid a \in \mathcal{A} \bigr\}$ to one of $k$ clusters $C_1, \dots, C_k$ such that the fastest runtimes for the benchmark instance $e$ are in cluster $C_1$ and the slowest are in cluster $C_{k-1}$.
Timeouts $\tau$ always form a separate cluster $C_{k}$.
The clustering is performed on a per-instance level as the best runtimes may be seconds on one instance and minutes on another.
After clustering, the best runtimes are all in the same cluster.
The runtime transformation function $\gamma_k : {\mathcal{A} \times \mathcal{I}} \rightarrow \left\lbrace 1, \dots, k \right\rbrace$ is then specified as follows:
%
$$\gamma_k(a, e) = j ~\Leftrightarrow~ r(a, e) \in C_j$$
%
Empirical studies on portfolio solvers have demonstrated that discretization is an effective approach in practice~\cite{CollauttiMMO13,NgokoCT19}.
Section~\ref{sec:exdesign:disc-pred} presents a detailed analysis of the runtime transformation in our scenario, including the clustering approach.
In particular, the analysis demonstrates that our benchmarking approach achieves higher ranking accuracy when working with discrete runtime labels rather than raw runtimes.

\subsubsection{Ranking Solvers}
\label{sec:main:model:ranking}

To determine the rank of solvers, we apply PAR-2 scoring to the discrete runtime labels $\gamma_k(a, e)$ and obtain the adapted scoring function $s_k : \mathcal{A} \rightarrow [1, 2 \cdot k]$:
%
\begin{align}
  s_k(a) := \frac{1}{|\mathcal{I}|} \sum_{e \in \mathcal{I}} \gamma'_k(a, e)
  &&
  \gamma'_k(a, e) := \begin{cases}
    2 \cdot \gamma_k(a, e)   & \text{if } \gamma_k(a, e) = k\\
  \gamma_k(a, e)  & \text{otherwise}
  \end{cases}
  \label{eq:rankingeq}
\end{align}
%
In this way, timeouts are still penalized.

\subsection{Instance Selection}
\label{sec:main:selection}

Selecting an instance based on the prediction model is a core functionality of our framework (cf.~Algorithm~\ref{algALBenchmark}, Line~4).
In this section, we introduce two instance sampling strategies, one that minimizes uncertainty and one that maximizes information gain.
Both strategies use the model's prediction function~$f$ and are inspired by existing work on active learning~\cite{settles2009active}.
These methods require the model's predictions to include probabilities for the $k$ discrete runtime labels.
Let $f_{\hat a}' : \mathcal{I} \rightarrow \left[0, 1\right]^k$ denote this modified prediction function.
In the following, the set $\tilde{\mathcal{I}} \subseteq \mathcal{I}$ denotes the instances that have already been sampled.

\paragraph{Uncertainty Sampling}

The uncertainty sampling strategy selects the instance closest to the model's decision boundary.
This is achieved by selecting the instance $e \in \mathcal{I} \setminus \tilde{\mathcal{I}}$ that minimizes the certainty~$U(e)$, which is specified by the following equation:
%
\begin{equation*}
  \operatorname{U}(e) := \left\lvert \frac{1}{k} - \max_{n \in \left\lbrace 1, \dots, k \right\rbrace} f_{\hat{a}}'(e)_{n} \right\rvert \text{.}
\end{equation*}
%
Intuitively, uncertainty sampling selects the instances where it is most unclear in which runtime cluster the new solver resides.
For example, if the new solver behaves similarly to two known solvers in the already run experiments, it is likely that this sampling strategy selects instances where these two solvers have runtimes that are not within the same cluster.

\paragraph{Information-Gain Sampling}

The information-gain sampling strategy selects the instance with the highest expected entropy reduction regarding the runtime labels.
To be more precise, we select the instance $e \in \mathcal{I} \setminus \tilde{\mathcal{I}}$ that maximizes $IG(e)$, which is specified in the following equation:
%
\begin{equation*}
  \operatorname{IG}(e) := \operatorname{H}(e) - \sum_{n = 1}^{k} f_{\hat{a}}'(e)_{n} \operatorname{\hat H}(e)_n \text{.}
\end{equation*}
%
In the equation, $\operatorname{H}(e)$ denotes the entropy of the runtime labels $\gamma(a, e)$ over all $a \in \mathcal{A}$ and $\operatorname{\hat H}(e)$ denotes the entropy of these labels plus the runtime label for $\hat{a}$.
The term $\operatorname{\hat H}(e)_n$ is computed for every possible runtime label $n \in \{1, \dots, k\}$.
Intuitively, this strategy favors instances where the most likely predicted cluster labels (as determined by $f_{\hat a}'$) would provide the most insight regarding similarities to known solvers.
For example, on a particular instance, one group of solvers might run into timeouts and another group of solvers might return a solution.
In this case, the information-gain strategy will likely select this instance as it provides insight about the group of solvers the new solver is similar to.

\subsection{Stopping Criteria}
\label{sec:main:stopping}

In this section, we present the two dynamic stopping criteria (cf.~Algorithm~\ref{algALBenchmark}, Line~3) employed in our experiments: the Wilcoxon and the ranking stopping criterion.
In the worst case, both criteria sample all instances, but typically they terminate earlier.
How early they terminate depends on how easy it is to determine the rank of the new solver among the existing ones.

\paragraph{Wilcoxon Stopping Criterion}

The Wilcoxon stopping criterion terminates the active-learning process based on the confidence that the predicted runtime labels of the new solver are sufficiently different from those of existing solvers.
This criterion is loosely inspired by Matricon et~al.~\cite{MatriconAFSH21}.
To assess the statistical significance of the predicted runtime labels, the criterion uses the average $p$-value $W_{\hat{a}}$ of a Wilcoxon signed-rank test $\operatorname{w}(S,P)$ of the runtime label distributions ${S=\{ \gamma(a, e) \mid e \in \mathcal{I} \}}$ for an existing solver $a$ and \mbox{$P=\{ f_{\hat a}(e) \mid e \in \mathcal{I} \}$} of the new solver $\hat{a}$:
%
\begin{equation*}
  W_{\hat{a}} := \frac{1}{\lvert \mathcal{A} \rvert} \sum_{a \in \mathcal{A}} \operatorname{w}(S, P)
\end{equation*}
%
To improve the stability of this criterion and mitigate the impact of outliers, we employ an exponential moving average~$W^{(i)}_{\exp}$:
%
\begin{align*}
  W_{\exp}^{\left(0\right)} &:= 1\\
  W_{\exp}^{\left(i\right)} &:= \beta W_{\hat{a}} + \left(1 - \beta\right) W_{\exp}^{\left(i - 1\right)}
\end{align*}
%
Active learning terminates when the value of $W^{(i)}_{\exp}$ drops below a fixed threshold.

\paragraph{Ranking Stopping Criterion}

In contrast, the ranking stopping criterion is less sophisticated than the previous one.
In this case, active learning terminates when the ranking induced by the model's predictions (cf.~Equation~\eqref{eq:rankingeq}) has remained unchanged within the last~$l$~iterations.
While the concrete values of the predicted score $s_{\hat a}$ may still change, the induced ranking is of sole interest in this criterion.

\section{Experimental Design}
\label{sec:exdesign}

This section outlines the experimental design employed in the presented study.
This includes an overview of the evaluation framework (cf.~Section~\ref{sec:exdesign:eval}), the datasets employed (cf.~Section~\ref{sec:exdesign:data}), the hyperparameter configuration (cf.~Section~\ref{sec:exdesign:hyper}), the runtime discretization (cf.~Section~\ref{sec:exdesign:disc-pred}), and the implementation (cf.~Section~\ref{sec:exdesign:impl}).

\subsection{Evaluation Framework}
\label{sec:exdesign:eval}

\begin{algorithm}[tb]
  \caption{Evaluation Framework}
  \label{alg:eval}

  \KwIn{Solvers $\mathcal{A}$, Instances $\mathcal{I}$, Runtimes $r : \mathcal{A} \times \mathcal{I} \rightarrow [0, \tau]$}
  \KwOut{Average Ranking Accuracy $\bar{O}_{\operatorname{acc}}$, Average Fraction of Runtime $\bar{O}_{\operatorname{rt}}$}
  \BlankLine

  $O \leftarrow \emptyset$
  % \BlankLine

  \For{$\hat{a} \in \mathcal{A}$}{
    $\mathcal{A}' \leftarrow \mathcal{A} \setminus \left\lbrace \hat{a} \right\rbrace$ \;
    $(s_{\hat a}, \mathcal{R}) \leftarrow \operatorname{runALAlgorithm}(\mathcal{A}', \mathcal{I}, r, \hat{a})$ \tcp*{Refer to Algorithm~\ref{algALBenchmark}}

  \BlankLine
  \tcp{Determine Ranking Accuracy}
    $O_{\operatorname{acc}} \leftarrow 0$ \;    
    \For{$a \in \mathcal{A}'$}{
      \If(\tcp*[f]{Same sign/order}){$\operatorname{sgn}(s_k(a) - s_{\hat a}) = \operatorname{sgn}(\operatorname{par_2}(a) - \operatorname{par_2}(\hat a))$}{
           $O_{\operatorname{acc}} \leftarrow O_{\operatorname{acc}} + \frac{1}{|\mathcal{A}'|}$ \;
      }
    }
  
  \BlankLine
  \tcp{Determine Runtime Fraction}
  $t_{\text{total}} \leftarrow \sum\limits_{e \in \mathcal{I}} r(\hat a, e)$\;
  $O_{\operatorname{rt}} \leftarrow 0$\;
    \For{$e \in \mathcal{I}$}{
    \If{$\exists t, (e,t) \in \mathcal{R}$}{
      $O_{\operatorname{rt}} \leftarrow O_{\operatorname{rt}} + \frac{t}{t_{\text{total}}}$\;
    }
  }
  
  % \BlankLine
    $O \leftarrow O \cup \bigl\{ ( O_{\operatorname{acc}},\, O_{\operatorname{rt}} ) \bigr\}$
  }

  % \BlankLine
  $\bigl( \bar{O}_{\operatorname{acc}}, \bar{O}_{\operatorname{rt}} \bigr) \leftarrow \operatorname{average}(O)$ \;
  
  % \BlankLine
  \Return $\bigl( \bar{O}_{\operatorname{acc}}, \bar{O}_{\operatorname{rt}} \bigr)$
\end{algorithm}

To assess our active-learning framework, we perform cross-validation over a set of solvers (cf.~Algorithm~\ref{alg:eval}).
Each solver assumes the role of the new solver~$\hat{a}$ once (Line~2) and is therefore excluded from the set of solvers $\mathcal{A}$ in that iteration (Line~3).
After running active learning for solver~$\hat{a}$ (Line~4), we compute the value of two optimization goals, i.e., ranking accuracy and runtime.
The \emph{ranking accuracy} $O_{\operatorname{acc}} \in \left[0, 1\right]$ (higher is better) is defined as the fraction of correctly ranked solver pairs~$\left(\hat{a}, a\right)$ regarding the PAR-2 scoring, where $a$ belongs to the set of all solvers $\mathcal{A}$ (Lines~5--8).
The \emph{fraction of runtime} required by the framework to reach its conclusion is represented by $O_{\operatorname{rt}} \in [0, 1]$ (lower is better).
This metric compares the accumulated runtimes over the sampled instances to the accumulated runtimes over all instances in the dataset (Lines~9--13).
Once all cross-validation results have been collected (Line~14), the output metrics are averaged and returned (Lines~15--16).

To compare different instantiations of our active-learning framework holistically, we insert the cross-validation results into Equation~\eqref{eq:opt}:
%
\begin{equation}
	O_\delta := \delta O_{\operatorname{acc}} + \left(1 - \delta\right) \left(1 - O_{\operatorname{rt}}\right) \enspace \textrm{,}
	\label{eq:opt}
\end{equation} 
%
$\delta \in \left[0, 1\right]$ is a factor for weighting the two optimization goals $O_{\operatorname{acc}}$ and $O_{\operatorname{rt}}$.
Plotting the approaches that maximize $O_\delta$ for all $\delta \in \left[0, 1\right]$ on an $O_{\operatorname{rt}}$-$O_{\operatorname{acc}}$-diagram provides us with a Pareto front of the best approaches for different optimization-goal weightings.

\subsection{Data}
\label{sec:exdesign:data}

In our experiments, we work with the dataset of the SAT~Competition~2022 Anniversary Track~\cite{sat2022,zenodoanniversarytrack}.
This dataset consists of 5355 instances with corresponding runtimes of 28 sequential SAT solvers.
For predictions within our framework, we also use a database of 56 instance features from the Global Benchmark Database~(GBD)~\cite{IserJ24}.
Refer to Appendix~\ref{sec:appendix-features} for a full list of all 56 features.
They include instance size features and node distribution statistics for several graph representations of SAT instances, and are primarily inspired by the SATzilla~2012 features described in~\cite{features}.
All features are numeric and have no missing values.
We drop 10 of the 56 features due to zero variance.
In total, the prediction models have access to 46 instance features and 27 out of 28 runtime features, excluding the respective new solver~$\hat{a}$.

In addition, we retrieve instance family information\footnote{\url{https://benchmark-database.de/getdatabase/meta}} to evaluate the composition of our sampled benchmarks.
Instance families consist of instances from the same application domain, such as planning, cryptography, etc., and are valuable for analyzing solver performance.
For hyperparameter tuning, we randomly sample \SI{10}{\%} of the complete set of 5355 instances with stratification regarding the instance family.
All instance families that are too \emph{small}, i.e., families with less than 10 instances, are put into one meta-family for stratification.
This \emph{tuning dataset} allows a more extensive exploration of the hyperparameter space.

\subsection{Hyperparameters}
\label{sec:exdesign:hyper}

There are several possible instantiations for the three subroutines \emph{ranking}, \emph{selection}, and \emph{stopping} in Algorithm~\ref{algALBenchmark}.
We describe these experimental configurations next.

\subparagraph{Ranking}

Regarding \emph{ranking} (cf.~Section~\ref{sec:main:model}), we experiment with the following instantiations:

\begin{itemize}\setlength{\itemsep}{1pt}
  \item Observed PAR-2 ranking of already sampled instances, without using prediction
  \item Predicted runtime-label ranking
  \vspace*{-1ex}
  \begin{itemize}\setlength{\itemsep}{1pt}
    \item
    History size: For stability, consider the latest 1, 10, 20, 30, or 40 predictions within a voting approach.
    The latest $x$ predictions for each instance vote on the instance's winning label.
    \item
    Fallback threshold: If the difference of scores between the new solver~$\hat{a}$ and another solver drops below \SI{0.01}, \SI{0.05}, or \SI{0.1}, use the partially observed PAR-2 ranking as a tie-breaker.
  \end{itemize}
\end{itemize}

\subparagraph{Selection}

We experiment with the following instantiations of the instance \emph{selection} (cf. Section~\ref{sec:main:selection}).
Since the potential runtime of the solver experiments is orders of magnitude larger than the model update time, we increment our benchmark one instance at a time rather than using batches of multiple instances.
Both these strategies are used in the state-of-the-art in active learning~\cite{SinhaED19,2019gaal}.
A drawback of selecting instances one at a time is the lack of parallel execution of runtime experiments.

\begin{itemize}\setlength{\itemsep}{1pt}
  \item Random sampling 
  \item Uncertainty sampling
  \vspace*{-1ex}
  \begin{itemize}\setlength{\itemsep}{1pt}
    \item Fallback threshold: Use random sampling for the first \SI{0}{\%}, \SI{5}{\%}, \SI{10}{\%}, \SI{15}{\%}, or \SI{20}{\%} of instances to explore the instance space.
    \item Runtime scaling: Whether to scale the uncertainty scores by the average runtime of solvers per instance or use $\operatorname{U}(e)$ as is.
  \end{itemize}

  \item Information-gain sampling
  \vspace*{-1ex}
  \begin{itemize}
    \item Fallback threshold: Use random sampling for the first \SI{0}{\%}, \SI{5}{\%}, \SI{10}{\%}, \SI{15}{\%}, or \SI{20}{\%} of instances to explore the instance space.
    \item Runtime scaling: Whether to scale the information-gain scores by the average runtime of solvers per instance or use $\operatorname{IG}(e)$ as is.
  \end{itemize}
\end{itemize}

\subparagraph{Stopping}

We evaluate the following \emph{stopping} (cf. Section~\ref{sec:main:stopping}) criteria:

\begin{itemize}\setlength{\itemsep}{1pt}
  \item Subset-size stopping criterion: Stop after sampling \SI{10}{\%} or \SI{20}{\%} of instances.
  \item Ranking stopping criterion
  \vspace*{-1ex}
  \begin{itemize}\setlength{\itemsep}{1pt}
    \item Minimum amount: Sample at least \SI{2}{\%}, \SI{8}{\%}, \SI{10}{\%}, or \SI{12}{\%} of instances before applying the criterion.
    \item Convergence duration: Stop if the predicted ranking stays the same for a number of sampled instances equal to \SI{1}{\%} or \SI{2}{\%} of all instances.
  \end{itemize}
  \item Wilcoxon stopping criterion
  \vspace*{-1ex}
  \begin{itemize}\setlength{\itemsep}{1pt}
    \item Minimum amount: Sample at least \SI{2}{\%}, \SI{8}{\%}, \SI{10}{\%}, or \SI{12}{\%} of instances before applying the criterion.
    \item Average of $p$-values to drop below: \SI{5}{\%}.
    \item Exponential moving average: Incorporate previous significance values by using an EMA with $\beta = 0.1$ or $\beta = 0.7$.
  \end{itemize}
\end{itemize}

\subsection{Runtime Discretization and Prediction}
\label{sec:exdesign:disc-pred}

One of the most important parts of our framework is the solver runtime model (cf.~Section~\ref{sec:main:model}).
In this section, we detail how clustering solver runtimes can lead to informative runtime labels and which machine learning models are best at predicting them.
To this end, we analyze prediction performance for estimating runtimes, timeouts, and discretized runtimes.

\paragraph{Regressing Runtime}

\begin{table}[tb]
	\centering
	\caption{Test-set prediction performance for predicting solver runtime (a continuous target) with two prediction models.}
	\label{tab:regression}
	\begin{tabular}{lcc}
		\toprule
		{Regression Models} & {Avg. RMSE ($\pm$ std)} & {Avg. MAE ($\pm$ std)} \\
		\midrule
		Random Forest & \SI{1929.84}{s} ($\pm$ \SI{1161.15}{s}) & \phantom{0}\SI{957.36}{s} ($\pm$ \SI{297.21}{s}) \\[0.4ex]
		Multilayer Perceptron & \SI{2275.20}{s} ($\pm$ \SI{1545.01}{s}) & \SI{1284.30}{s} ($\pm$ \SI{390.57}{s}) \\
		\bottomrule
	\end{tabular}
\end{table}

As a baseline, Table~\ref{tab:regression} shows the prediction performance of two regression models, i.e., a random forest and a multilayer perceptron.
We employ these models to estimate a new solver's raw runtime as the prediction target.
To this end, we use 46 instance features (cf.~Section~\ref{sec:exdesign:data}) and 27 known solver runtimes as inputs for the predictions.
For this experiment, we split the data into a training set (80\% of all data) on which the target runtime is known and a test set (20\% of all data) on which we predict the runtimes of the new solver.
We repeat this evaluation for each solver as the prediction target and report the average and standard deviation of prediction performance over the 28 solvers in Table~\ref{tab:regression}.

We chose both types of prediction models for their robustness and ability to capture nonlinear dependencies~\cite{breiman2001random}.
However, both still perform poorly, as evidenced by the magnitudes of the mean regression errors in Table~\ref{tab:regression}.
In particular, a root mean squared error (RMSE) of about \SI{2000}{s} is relatively high for solver runtimes that range from 0 to \SI{5000}{s}.
Such unreliable runtime estimates could affect active learning negatively.
Therefore, we decided to discretize the runtimes for prediction, moving from a regression scenario to a classification scenario.

\paragraph{Classifying Timeouts}

\begin{table}[tb]
	\centering
	\caption{Test-set prediction performance for predicting solver timeouts (a binary target) with different prediction models.}
	\label{tab:timeout-prediction}
	~\\[1em]
	\begin{tabular}{lcc}
		\toprule
		\multirow[c]{2}{*}[-0.27em]{Timeout Prediction Models} & \multicolumn{2}{c}{Average MCC ($\pm$ std)} \\
		\cmidrule(lr){2-3}
		& {Without Runtime Feat.} & {With Runtime Feat.} \\
		\midrule
		Stacking (QDA\,+\,RF)                 & 0.6513 ($\pm$ 0.0458) & 0.9527 ($\pm$ 0.0292) \\[0.4ex]
		Quadratic Discriminant Analysis (QDA)                    & 0.2593 ($\pm$ 0.1310) & 0.9290 ($\pm$ 0.0339) \\[0.4ex]
		Random Forest (RF)         & 0.6607 ($\pm$ 0.0547) & 0.8530 ($\pm$ 0.0479) \\[0.4ex]
		AdaBoost                             & 0.5412 ($\pm$ 0.0612) & 0.8384 ($\pm$ 0.0444) \\[0.4ex]
		Decision Tree                         & 0.5980 ($\pm$ 0.0423) & 0.8059 ($\pm$ 0.0707) \\[0.4ex]
		Logistic Regression                   & 0.2031 ($\pm$ 0.0728) & 0.8052 ($\pm$ 0.1018) \\[0.4ex]
		$k$NN                   & 0.5108 ($\pm$ 0.0387) & 0.7885 ($\pm$ 0.1521) \\[0.4ex]
		Multilayer Perceptron                      & 0.1293 ($\pm$ 0.0718) & 0.7760 ($\pm$ 0.1408) \\[0.4ex]
		Support Vector Machine               & 0.0595 ($\pm$ 0.0554) & 0.7757 ($\pm$ 0.2149) \\[0.4ex]
		Naive Bayes                           & 0.1173 ($\pm$ 0.0872) & 0.7306 ($\pm$ 0.1394) \\
		\bottomrule
	\end{tabular}
\end{table}

Table~\ref{tab:timeout-prediction} shows the prediction performance for estimating whether a solver times out or not.
This binary prediction target represents a very strong discretization of runtimes.
We compare a variety of prediction models that pursue different learning paradigms.
Each is trained with either instance features only (second column) or instance and runtime features (third column).
For this experiment, we split the data into a training set (80\% of all data) on which the target label is known and a test set (20\% of all data) on which we predict the runtime label of the new solver.
As the evaluation metric for prediction performance, we use Matthews Correlation Coefficient (MCC)~\cite{gorodkin2004comparing, matthews1975comparison}, which reaches its maximum of~1 for a perfect prediction, is~0 for random guessing, and has a minimum of~-1.

Training the classification models with instance features only (as well as the target labels on the training set), a random forest performs best in predicting the target labels on the test set.
In contrast, a quadratic discriminant analysis performs poorly in this scenario but performs very well if runtime features are included.
However, the best prediction performance with all features (as well as the target labels on the training set) is achieved by a stacking ensemble~\cite{wolpert1992stacked} consisting of the quadratic discriminant analysis~\cite{tharwat2016linear} and the random forest~\cite{breiman2001random}.
Stacking means that another model, in our case a simple decision tree~\cite{breiman1984classification}, acts as a meta-model:
The prediction outputs of the ensemble members, i.e., quadratic discriminant analysis and random forests, become the prediction inputs of the meta-model, i.e., decision tree.

\paragraph{Classifying Discretized Runtime Labels}

Building on the success of binary timeout prediction, we investigate a classification scenario where the target is solver runtime discretized into multiple categories.

\subparagraph{Label definition}

To define this prediction target, we partition the solver runtimes into $k$~clusters.
We conduct this clustering step per instance, i.e., the solvers forming a particular cluster may differ from instance to instance.
The adapted solver scoring function~$s_k$ (cf.~Equation~\eqref{eq:rankingeq}) subsequently uses the cluster labels.
In preliminary experiments, the most \emph{useful} labels were produced with hierarchical clustering and a log-single-linkage criterion; we will discuss our notion of label usefulness later.
Other clustering approaches we have tried include hierarchical clustering with mean-, median-, and complete-linkage criteria, as well as $k$-means and spectral clustering.

In our chosen hierarchical clustering procedure, each non-timeout runtime starts in a separate cluster.
We then gradually merge the closest clusters until the desired number of $k-1$ partitions is reached.
By definition, the $k$-th cluster always contains the timeouts and no other runtimes (cf.~Section~\ref{sec:main:model:transformation}).
To quantify the distance between two clusters, we consider the minimum pairwise difference of logarithmic runtimes between solvers from the two clusters.

\subparagraph{Label prediction}

\begin{table}[tb]
  \centering
  \caption{Test-set prediction performance for predicting the $C_k$ cluster labels (a discrete target) with the best-performing models.}
  \label{tab:cluster-prediction}
  \begin{tabular}{lcc}
    \toprule
    \multirow[c]{2}{*}[-0.27em]{$C_3$ Cluster Prediction Models} & \multicolumn{2}{c}{Average MCC ($\pm$ std)} \\
    \cmidrule(lr){2-3}
    & {Without Clustering Feat.} & {With Clustering Feat.} \\
    \midrule
    Stacking (QDA\,+\,RF)                 & 0.7464 ($\pm$ 0.0497) & 0.8380 ($\pm$ 0.0634) \\[0.4ex]
    Quadratic Discriminant Analysis (QDA)                     & 0.6903 ($\pm$ 0.0607) & 0.7738 ($\pm$ 0.0459) \\[0.4ex]
    Random Forest (RF)         & 0.7116 ($\pm$ 0.0385) & 0.7841 ($\pm$ 0.0469) \\[0.4ex]
    \midrule
    \multirow[c]{2}{*}[-0.27em]{$C_4$ Cluster Prediction Models} & \multicolumn{2}{c}{Average MCC ($\pm$ std)} \\
    \cmidrule(lr){2-3}
    & {Without Clustering Feat.} & {With Clustering Feat.} \\
    \midrule
    Stacking (QDA\,+\,RF)                 & 0.6562 ($\pm$ 0.0538) & 0.7366 ($\pm$ 0.0377) \\[0.4ex]
    Quadratic Discriminant Analysis (QDA)                     & 0.6007 ($\pm$ 0.0765) & 0.6872 ($\pm$ 0.0469) \\[0.4ex]
    Random Forest (RF)         & 0.6033 ($\pm$ 0.0455) & 0.6828 ($\pm$ 0.0527) \\[0.4ex]
    \midrule
    \multirow[c]{2}{*}[-0.27em]{$C_5$ Cluster Prediction Models} & \multicolumn{2}{c}{Average MCC ($\pm$ std)} \\
    \cmidrule(lr){2-3}
    & {Without Clustering Feat.} & {With Clustering Feat.} \\
    \midrule
    Stacking (QDA\,+\,RF)                 & 0.5835 ($\pm$ 0.0615) & 0.6396 ($\pm$ 0.0553) \\[0.4ex]
    Quadratic Discriminant Analysis (QDA)                      & 0.5508 ($\pm$ 0.0669) & 0.5881 ($\pm$ 0.0912) \\[0.4ex]
    Random Forest (RF)         & 0.5420 ($\pm$ 0.0459) & 0.5895 ($\pm$ 0.0475) \\
    \bottomrule
  \end{tabular}
\end{table}

Next, we use the runtimes labels from this discretization procedure as prediction targets.
Table~\ref{tab:cluster-prediction} shows the prediction performance for the best classification models from Table~\ref{tab:timeout-prediction} and $k \in \lbrace 3, 4, 5 \rbrace$ clusters.
We train the prediction models either with 46 instance features (cf.~Section~\ref{sec:exdesign:data}) and 27 known solver runtimes (second column) or additionally including 27 features representing the known $C_k$ cluster labels of other solvers (third column).
For this experiment, we split the data into a training set (80\% of all data) on which the target cluster label is known and a test set (20\% of all data) on which we predict the cluster label of the new solver.

For all prediction models and values of~$k$, the prediction performance is better when the cluster labels are included as features.
In addition, the prediction performance degrades rapidly as the number of clusters increases.
The stacking ensemble performs best in all cases, so we use it in all subsequent experiments.

\begin{figure}[tb]
	\centering
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[height=4.123cm]{plots/cm2labels.pdf}
		\caption{Timeout prediction.}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[height=4.228cm]{plots/cm3labels.pdf}
		\caption{$C_3$ cluster prediction.}
	\end{subfigure}
	\caption{Confusion matrices for predicting timeouts and $C_3$ cluster labels.}
	\label{fig:confusion-matrices}
\end{figure}

To analyze the stacking ensemble in more depth, Fig.~\ref{fig:confusion-matrices} shows the confusion matrices for predicting timeouts and $C_3$ cluster labels.
Both scenarios admit relatively good prediction performance, with the model for $C_3$ performing slightly worse than the model for timeouts.
Notably, for $C_3$, there is less confusion between the non-timeout labels (Labels~1 and~2) and the timeout label (Label~3) than between the non-timeout labels (Labels~1 and~2).
This observation suggests that deciding on timeouts is easier than distinguishing between discretized non-timeout runtimes.

\subparagraph{Label usefulness}

% TODO tf: Actual rankings with discretization
% * Correct ranking pairs in table below: 97.45%
% * Max. difference between wrong pairs 0.11
% * Spearman correlation of PAR2 and Discrete rank: 0.9862
\begin{table}[p]
	\centering
	\caption{Ranking of solvers for the PAR-2 score and our discretized $s_3$ score.}
	\label{tab:ranking}
	~\\[1em]
	\begin{tabular}{
			>{\centering\arraybackslash}m{0.09\textwidth}
			>{\centering\arraybackslash}m{0.15\textwidth}
			>{\centering\arraybackslash}m{0.09\textwidth}
			>{\centering\arraybackslash}m{0.15\textwidth}
			>{\raggedright\arraybackslash}m{0.01\textwidth}
			>{\raggedright\arraybackslash}m{0.314\textwidth}
		}
		\toprule
		\multicolumn{2}{c}{PAR-2} & \multicolumn{2}{c}{$s_3$} & & \multirow[c]{2}{*}{\vspace{-0.15cm}\hspace{-0.02cm}Solver} \\
		\cmidrule(r){1-2}\cmidrule(r){3-4}
		Rank &   Score & Rank &  Score & & \\
		\midrule
		1 & 2808.13 &    1 & 1.1717 & & kissat-mab-esa \\
		2 & 2812.93 &    2 & 1.1832 & & kissat-sc2022-bulky \\
		3 & 2835.25 &    3 & 1.1862 & & ekissat-mab-gb-db \\
		4 & 2835.59 &    4 & 1.1868 & & kissat-mab-ucb \\
		5 & 2836.92 &    5 & 1.1868 & & kissat-inc \\
		6 & 2845.19 &    6 & 1.1926 & & ekissat-mab-db-v1 \\
		7 & 2846.73 &    7 & 1.1930 & & kissat-mab-moss \\
		8 & 2857.67 &    8 & 1.1947 & & kissat-mab-hywalk \\
		9 & 2869.45 &    9 & 1.1998 & & kissat-sc2022-light \\
		10 & 2899.70 &   10 & 1.2164 & & kissat-els-v2 \\
		11 & 2953.59 &   11 & 1.2290 & & hkis-unsat \\
		12 & 2967.53 &   12 & 1.2347 & & kissat-adaptive-restart \\
		13 & 2976.56 &   13 & 1.2475 & & seqfrost-noextend \\
		14 & 3014.40 &   16 & 1.2645 & & kissat-els-v1 \\
		15 & 3017.73 &   14 & 1.2509 & & cadical-esa \\
		16 & 3036.83 &   15 & 1.2613 & & cadical-reorder \\
		17 & 3049.90 &   20 & 1.3648 & & cadical-rel-scavel \\
		18 & 3080.66 &   19 & 1.2965 & & kissat-relaxed \\
		19 & 3095.73 &   17 & 1.2815 & & cadical-dvdl-v1 \\
		20 & 3101.12 &   18 & 1.2856 & & cadical-dvdl-v2 \\
		21 & 3273.95 &   21 & 1.3786 & & glucose-reboot \\
		22 & 3290.90 &   25 & 1.4707 & & lstech-maple-hywalk \\
		23 & 3292.68 &   23 & 1.4478 & & lstech-maple \\
		24 & 3400.72 &   24 & 1.4693 & & slime-sc-2022-beta \\
		25 & 3412.11 &   26 & 1.5237 & & slime-sc-2022 \\
		26 & 3436.07 &   22 & 1.4161 & & hcad-v1-psids \\
		27 & 3506.32 &   27 & 1.5433 & & maple-lcmdistchrbt-dl-v3 \\
		28 & 4741.50 &   28 & 2.0581 & & isasat \\
		\bottomrule
	\end{tabular}
\end{table}

To ensure that the runtime labels also are \emph{useful} for active learning, we evaluate whether the labels still discriminate solvers and agree with the actual PAR-2 ranking.
Table~\ref{tab:ranking} shows the ranking of the solvers in the SAT Competition~2022 Anniversary Track~\cite{sat2022} according to the standard PAR-2 score and according to our discretized $s_3$ score.
We observe that ranking with our score correctly decides for almost all (about \SI{97.45}{\%}; $\sigma = \SI{3.68}{\%}$) solver pairs which solver is faster.
In particular, the Spearman correlation of $s_3$ and the PAR-2 score over all solvers is about \SI{0.988}{}, which is very close to the optimal value of~1~\cite{de2016comparing}.

% Amount of significant solver pairs (Wilcoxon signed-rank, 5% alpha):
% PAR-2:   684/756 = 0.9048
% timeout: 660/756 = 0.8730
% k=3:     662/756 = 0.8757
% k=4:     662/756 = 0.8757
% k=5:     662/756 = 0.8757
\begin{figure}[p]
	\centering
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=3.9cm]{plots/par2sigdiff.pdf}
		\caption{PAR-2 scores.}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=3.9cm]{plots/timeoutsigdiff.pdf}
		\caption{Timeouts.}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=3.9cm]{plots/s3sigdiff.pdf}
		\caption{$s_3$ scores.}
	\end{subfigure}
	\caption{Solver pairs with significant differences regarding their scorings are colored white, non-significant ones black. Solvers are ordered identically to Table~\ref{tab:ranking}.}
	\label{fig:solver-confusion-matrices}
\end{figure}

We also analyze how runtime discretization affects the distinguishability of solver pairs.
The matrices in Fig.~\ref{fig:solver-confusion-matrices} visualize which solver pairs exhibit significant differences in their PAR-2 scores, timeouts, and $s_3$ scores.
According to a Wilcoxon signed-rank test with a significance level of $\alpha = 0.05$, \SI{87.57}{\%} of the solver pairs have significantly different scores after $\gamma_3$-discretization, which is only a slight decrease compared to \SI{90.48}{\%} before discretization.
$\gamma_2$-discretization significantly distinguishes \SI{87.30}{\%} of solver pairs.
These results support the usefulness of discretized runtimes for our active learning framework.

\subsection{Implementation Details}
\label{sec:exdesign:impl}

For reproducibility, our source code and data are available on GitHub (see footnotes in Section~\ref{sec:intro}).
Our code is implemented in \textsc{Python} using \emph{scikit-learn}~\cite{scikit-learn} for making predictions, and \emph{gbd-tools}~\cite{IserJ24} for retrieving SAT instances.


\section{Evaluation}
\label{sec:eval}

In this section, we evaluate our active learning framework.
While we split the data into a fixed train and test set to evaluate performance in Section~\ref{sec:exdesign:disc-pred}, in this section, the target runtime labels of the new solver are only known for the experiments that have already been selected by active learning.
First, we analyze and tune the different subroutines of our framework on the tuning dataset (cf.~Section~\ref{sec:eval:hyper}).
Next, we evaluate the best configurations on the full dataset (cf.~Section~\ref{sec:eval:full}).
Finally, we analyze the importance of instance families for our framework (cf.~Section~\ref{sec:eval:instance}).

\subsection{Hyperparameter Analysis}
\label{sec:eval:hyper}

\begin{figure}[tbp!]
  \centering
  \begin{subfigure}{1.0\textwidth}
    \resizebox{\textwidth}{!}{
      \graphicspath{{plots/}}
      \input{plots/anni_train_color_ranking.pgf}
    }
    \caption{Ranking approaches.}
    \label{fig:annitraincolorranking}
  \end{subfigure}
  \\
  \vspace{0.2cm}
  \begin{subfigure}{1.0\textwidth}
    \resizebox{\textwidth}{!}{
      \graphicspath{{plots/}}
      \input{plots/anni_train_color_selection.pgf}
    }
    \caption{Selection approaches.}
    \label{fig:annitraincolorselection}
  \end{subfigure}
  \\
  \vspace{0.2cm}
  \begin{subfigure}{1.0\textwidth}
    \resizebox{\textwidth}{!}{
      \graphicspath{{plots/}}
      \input{plots/anni_train_color_stopping.pgf}
    }
    \caption{Stopping criteria.}
    \label{fig:annitraincolorstopping}
  \end{subfigure}
  \caption{
    $O_{\operatorname{rt}}$-$O_{\operatorname{acc}}$-diagrams (cf. Section~\ref{sec:exdesign:eval}) comparing different hyperparameter instantiations of our active-learning framework on the hyperparameter-tuning dataset.
    The x-axis shows the ratio of total solver runtime on the sampled instances relative to all instances.
    The y-axis shows the ranking accuracy.
    Each line represents the front of Pareto-optimal configurations for the respective hyperparameter instantiation.
  }
  \label{fig:e2eallsolvers}
\end{figure}

\subparagraph{Methodology}

Fig.~\ref{fig:e2eallsolvers} shows the performance of the hyperparameter configurations from Section~\ref{sec:exdesign:hyper} on $O_{\operatorname{rt}}$-$O_{\operatorname{acc}}$ plots (cf.~Section~\ref{sec:exdesign:eval}) for the hyperparameter-tuning dataset.
Evaluating one configuration with Algorithm~\ref{alg:eval} returns one point $\left(O_{\operatorname{rt}},\, O_{\operatorname{acc}}\right)$.
We do not show intermediate results of the active learning procedure, but only the final results after stopping.
The plotted lines represent the best-performing configurations per ranking approach (Fig.~\ref{fig:annitraincolorranking}), selection approach (Fig.~\ref{fig:annitraincolorselection}), and stopping criterion (Fig.~\ref{fig:annitraincolorstopping}).
In particular, we show the Pareto front, i.e., from all configurations that share a particular value of the plotted hyperparameter, we take the maximum ranking accuracy over all remaining hyperparameters \emph{not} shown in the corresponding plot.

\subparagraph{Ranking}

Regarding ranking approaches (Fig.~\ref{fig:annitraincolorranking}), the predicted runtime-label ranking outperforms the partially observed PAR-2 ranking consistently for every possible value of the tradeoff parameter~$\delta$.
This result is expected since selection decisions are usually not random.
For example, we might sample more instances of a particular family if it benefits the discrimination of solvers.
While such a sampling biases the partially observed PAR-2 score, the prediction model can account for this.

\subparagraph{Selection}

Regarding the selection approaches (Fig.~\ref{fig:annitraincolorselection}), uncertainty sampling performs best in most cases.
However, information-gain sampling is advantageous when runtime is strongly favored (small $\delta$; runtime fraction less than \SI{5}{\%}).
This result is consistent with our expectations:
Information-gain sampling selects instances that maximize the expected reduction in entropy.
This means that we sample instances that reveal similarities rather than differences between solvers, which helps to build a confident model quickly.
However, the method cannot select instances that are useful for distinguishing solvers later.
Random sampling performs reasonably well but is outperformed by uncertainty sampling in all cases, demonstrating the benefit of actively selecting instances based on a prediction model.

\subparagraph{Stopping}

Regarding the stopping criteria (Fig.~\ref{fig:annitraincolorstopping}), the ranking stopping criterion performs most consistently well.
When accuracy is strongly favored (very high $\delta$), the Wilcoxon stopping criterion performs better.
The subset-size stopping criterion performs reasonably well, but does not improve beyond a certain accuracy because it samples a fixed subset of instances, independent of prediction performance.

\begin{figure}[tb!]
  \centering
  \begin{subfigure}{0.4775\textwidth}
    \resizebox{\textwidth}{!}{
      \graphicspath{{plots/}}
      \input{plots/anni_train_optimization_goal.pgf}
    }
    \caption{Runtime vs. Instances}
    \label{fig:annitrainoptgoalruntime}
  \end{subfigure}
  \begin{subfigure}{0.5125\textwidth}
    \resizebox{\textwidth}{!}{
      \graphicspath{{plots/}}
      \input{plots/anni_train_delta_acc.pgf}
    }
    \caption{Runtime vs. Accuracy}
    \label{fig:annitrainoptgoalacc}
  \end{subfigure}

  \caption{
    Scatter plot comparing different instantiations of the tradeoff parameter~$\delta$ for our active learning framework on the hyperparameter-tuning dataset.
    The x-axes show the fraction of runtime $O_{\operatorname{rt}}$ of the sample, while the y-axes show the fraction of sampled instances and the ranking accuracy, respectively.
    The color indicates the weighting between different optimization objectives $\delta \in \left[0, 1\right]$.
    The larger $\delta$, the more we favor accuracy over runtime.
  }
  \label{fig:annitrainoptgoal}
\end{figure}

\subparagraph{Objective weighting}

Fig.~\ref{fig:annitrainoptgoalruntime} shows an interesting consequence of weighting our optimization goals:
On the one hand, if we want to get a \emph{rough} estimate of a solver's performance quickly (low~$\delta$), our framework favors selecting many \emph{easy} instances.
In particular, the fraction of sampled instances is larger than the fraction of runtime.
By having many observations, it is easier to build a model.
On the other hand, if we want to get a \emph{good} estimate of a solver's performance in a moderate amount of time (high~$\delta$), our framework favors selecting few, \emph{difficult} instances.
In particular, the fraction of instances is smaller than the fraction of runtime.
Furthermore, Fig.~\ref{fig:annitrainoptgoalacc} shows which values of $\delta$ make the most sense.
The range $\delta \in \left[0.2, 0.8\right]$ corresponds to the points with a runtime fraction between \SI{0.03} and \SI{0.22}.
We consider this region to be the most promising, analogous to the \emph{elbow} method in cluster analysis~\cite{kodinariya2013review}.

\subsection{Full-Dataset Evaluation}
\label{sec:eval:full}

After selecting the most promising hyperparameters, we run our active learning experiments on the complete Anniversary Track dataset (5355 instances).
The above range $\delta \in \left[0.2, 0.8\right]$ results in only two different hyperparameter configurations.
The best-performing approach for $\delta \in \left[0.2, 0.7\right]$ uses the predicted runtime label ranking, information-gain sampling, and ranking stopping criterion.
It can predict the PAR-2 ranking of a new solver with \SI{90.48}{\%} accuracy ($O_{\operatorname{acc}}$) in only \SI{5.41}{\%} of the total evaluation time ($O_{\operatorname{rt}}$).
The best-performing approach for $\delta \in (0.7, 0.8]$ uses the predicted runtime label ranking, uncertainty sampling, and ranking stopping criterion.
It can predict the PAR-2 ranking of a new solver with \SI{92.33}{\%} accuracy~($O_{\operatorname{acc}}$) in only \SI{10.35}{\%} of the total evaluation time~($O_{\operatorname{rt}}$).

\begin{table}[tp]
  \centering
  \caption{
    Performance comparison (on the full dataset) of the best-performing AL approach (\emph{AL}), random sampling of the same runtime fraction with 1000 repetitions (\emph{Random}), and static selection of the instances most frequently sampled by active learning approaches (\emph{Most Frequent}).
  }
  \label{tab:fulldataset}
  \begin{tabular}{
    >{\arraybackslash}p{0.4\textwidth}
    S[table-format=2.2,table-column-width=0.15\textwidth]
    S[table-format=2.2,table-column-width=0.15\textwidth]
    S[table-format=2.2,table-column-width=0.15\textwidth]
  }
    \toprule
    {For $\delta \in \left[0.2, 0.7\right]$} & {AL} & {Random} & {Most Frequent} \\
    \midrule
    Sampled Runtime Fraction (\%) & 5.41 & 5.43 & 5.44 \\
    Sampled Instance Fraction (\%) & 26.53 & 5.43 & 27.75 \\
    Ranking Accuracy (\%) & 90.48 & 88.54 & 81.08 \\
    \midrule
    {For $\delta \in (0.7, 0.8]$}  & {AL} & {Random} & {Most Frequent} \\
    \midrule
    Sampled Runtime Fraction (\%) & 10.35 & 10.37 & 10.37 \\
    Sampled Instance Fraction (\%) & 5.24 & 10.37 & 36.96 \\
    Ranking Accuracy (\%) & 92.33 & 91.61 & 84.52 \\
    \bottomrule
  \end{tabular}
\end{table}

Table~\ref{tab:fulldataset} shows how these two active learning approaches (column \emph{AL}) compare to two static baselines:
\emph{Random} samples instances until it reaches roughly the same fraction of runtime as the AL benchmark sets.
We repeat the sampling 1000 times and report the average results.
\emph{Most Frequent} uses a static benchmark set consisting of the instances most frequently sampled by our active learning approach.
In particular, we consider the average sampling frequency across all solvers and Pareto-optimal active learning approaches.

Both of our AL approaches outperform random sampling.
However, the performance differences are not significant for a Wilcoxon signed-rank test with $\alpha = 0.05$ and also depend on the fraction of the runtime sampled (cf.~Fig.~\ref{fig:annitraincolorselection}).
However, an advantage of our approach is that it indicates when to stop adding new instances.
Recall that Fig.~\ref{fig:annitrainoptgoalacc} shows the trade-off of the number of sampled instances and the ranking accuracy.
Active learning helps to identify the optimal region as sampling too many instances only provides marginal improvements compared to its cost.

A static benchmark utilizing the most frequently AL-sampled instances exhibits suboptimal performance compared to active learning and random sampling.
This outcome is anticipated since the static benchmark fails to reflect an appropriate balance of instance families.

\subsection{Instance Frequency and Instance-Family Importance}
\label{sec:eval:instance}

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.6\textwidth]{plots/instoccs.pdf}
  \caption{Frequency with which individual problem instances are selected within all active-learning benchmarks. Instances that are chosen more frequently are intuitively more important for distinguishing solver runtimes. Results are on the full dataset with cross-validation over all solvers.}
  \label{fig:instoccs}
\end{figure}

\begin{figure}[tb]
  \centering
  \resizebox{0.75\textwidth}{!}{
    \graphicspath{{plots/}}
    \input{plots/anni_final_families.pgf}
  }
  \caption{
    Scatter plot illustrating the relative importance of different instance families for ranking on the full dataset. The x-axis depicts the frequency of instance families in the dataset, while the y-axis represents the average frequency in the samples selected by active learning. The dashed line represents families that occur with the same frequency in the dataset and samples. Results are on the full dataset with cross-validation over all solvers.
  }
  \label{fig:annifinalfamilies}
\end{figure}

Fig.~\ref{fig:instoccs} illustrates the frequency distribution with which individual instances are selected across all active learning benchmarks regarding the full dataset with cross-validation over all solvers.
A small proportion of instances is selected with great frequency, while most instances are selected on only a few occasions.
This indicates that there is no fixed subset of instances that can perfectly distinguish all solvers, which highlights the value of employing an active-learning-based strategy.

The selection decisions of our approach also reveal the importance of instance families for our framework.
Fig.~\ref{fig:annifinalfamilies} illustrates the occurrence of instance families within the dataset and the benchmarks created by active learning.
We use the best-performing configurations for all values of $\delta \in \left[0, 1\right]$.
While most families exhibit the same fraction in the dataset and the sampled benchmarks, a few outliers require further discussion.
Instances of the families \emph{fpga}, \emph{quasigroup-completion}, and \emph{planning} are particularly useful for distinguishing solvers within our framework.
These instances are selected in greater proportion than in the full dataset.
In contrast, instances of the largest family, i.e., \emph{hardware-verification}, appear with approximately the same fraction in the dataset and the sampled benchmarks.
Finally, instances of the family \emph{cryptography} are less important in distinguishing solvers than their substantial representation in the dataset would suggest.
One possible explanation for this discrepancy is that these instances are highly similar, such that a small fraction of them is sufficient to estimate a solver's performance on all of them.

\section{Conclusions and Future Work}
\label{sec:conclusion}

In this work, we have addressed the \emph{New-Solver Problem}:
Given a new solver, we aim to determine its ranking relative to competitors.
Our approach provides accurate ranking predictions while requiring significantly less runtime than a complete evaluation on a given benchmark set.
On data from the SAT~Competition~2022 Anniversary Track, we can determine a new solver's PAR-2 ranking with about \SI{92}{\%} accuracy while only requiring \SI{10}{\%} of the full-evaluation time.
We have evaluated several ranking methods, instance-selection approaches, and stopping criteria within our sequential active learning framework.
We have also looked at which instance families are the most prevalent in selection decisions.

Future work could compare more sub-routines for ranking, instance selection, and stopping.
Further, our evaluation framework can be used for other computation-intensive problems.
In particular, many $\mathcal{NP}$-complete problems share most of the relevant properties of SAT, like established instance features, a complete benchmark is expensive, and traditional benchmark selection requires expert knowledge.

From a technical perspective, we could formulate the problem of runtime discretization as an optimization problem rather than addressing it empirically.
Further, a major shortcoming of our current approach is the lack of parallelization, selecting instances one at a time.
Benchmarking on a computing cluster with $n$ cores benefits from having batches of $n$ instances.
However, bigger batch sizes $n$ impede \emph{active learning}.
Also, it needs to be clarified how to synchronize instance selection and updates of the prediction model without wasting too much runtime.

\backmatter

\section*{Declarations}

\bmhead{Funding}

This work was supported by the Ministry of Science, Research and the Arts Baden-Württemberg, project \emph{Algorithm Engineering for the Scalability Challenge (AESC)}.

\bmhead{Competing interests}

The authors have no competing interests to declare that are relevant to the content of this article.

\bmhead{Availability of data and materials}

All experimental data are available online at \url{https://github.com/mathefuchs/al-for-sat-solver-benchmarking-data}.

\bmhead{Code availability}

The code is available online at \url{https://github.com/mathefuchs/al-for-sat-solver-benchmarking}.

\bibliography{literature}

%
% ---- Appendix ----
%

\newpage
\appendix
\section{Instance Features}
\label{sec:appendix-features}

For predictions within our framework, we use a database of 56 instance features\footnote{\url{https://udopia.github.io/gbdc/doc/extractors/CNFBaseFeatures.html}} from the Global Benchmark Database~(GBD)~\cite{IserJ24}.
Table~\ref{tab:features} briefly summarizes all 56 features.
The placeholder \texttt{<n>} can take values within the range from 1 to 9, and the placeholder \texttt{<dist>} is one of \texttt{mean}, \texttt{variance}, \texttt{min}, \texttt{max}, or \texttt{entropy}.
Accounting for all possible instantiations of \texttt{<n>} and \texttt{<dist>}, there are 56 features in total.

\begin{table}[h]
    \centering
    \caption{
      List of all 56 instance features, where \texttt{<dist>} is one of \texttt{mean}, \texttt{variance}, \texttt{min}, \texttt{max}, or \texttt{entropy}.
    }
    \label{tab:features}
    \begin{tabular}{
      >{\arraybackslash}p{0.22\textwidth}
      >{\arraybackslash}p{0.21\textwidth}
      >{\arraybackslash}p{0.46\textwidth}
    }
      \toprule
      Category & Feature Name & Description \\
      \midrule
      Basic statistics & \verb|clauses| & Number of clauses \\
      & \verb|variables| & Number of variables \\
      \midrule
      Clause types & \verb|cls<n>| & Number of clauses with \verb|<n>| literals for \verb|<n>| in 1--9 \\
      & \verb|cls10p| & Number of clauses with 10 or more literals \\
      & \verb|horn| & Number of Horn clauses \\
      & \verb|invhorn| & Number of inverted Horn clauses \\
      & \verb|positive| & Number of positive clauses \\
      & \verb|negative| & Number of negative clauses \\
      \midrule
      Horn proximity & \verb|hornvars_<dist>| & Distribution of variable occurrence in Horn clauses \\
      & \verb|invhornvars_<dist>| & Distribution of variable occurrence in inverted Horn clauses \\
      \midrule
      Polarity balance & \verb|balancecls_<dist>| & Distributions of fractions of number of positive/negative literals in clauses \\
      & \verb|balancevars_<dist>| & Distributions of fractions of number of positive/negative literals of variables \\
      \midrule
      Variable-clause graph & \verb|vcg_vdegree_<dist>| & Variable degree distribution \\
      & \verb|vcg_cdegree_<dist>| & Clause degree distribution \\
      \midrule
      Variable graph & \verb|vg_degree_<dist>| & Degree distribution \\
      \midrule
      Clause graph & \verb|cg_degree_<dist>| & Degree distribution \\
      \bottomrule
    \end{tabular}
\end{table}

\end{document}
